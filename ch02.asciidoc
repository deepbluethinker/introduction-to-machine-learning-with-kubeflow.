[[simple_training_ch]]
== New stuff starts here- (Trevo's refactor)

== Hello Kubeflow

Introduction

=== Setup

We assume the user has installed, Helm, Tiller, KSonnet, and Argo. If you don't have all of these requirements already installed,
please install them first. TODO: Trevor- Version?

==== Creating a Kubeflow project (H)

Before we get started, we will need to make a Kubeflow project for us to work in.
The script `kfctl.sh`, in the scripts directory of your Kubeflow installation,
allows you to create projects. We'll start with an example project in <<create_example_project>>.

[[create_example_project]]
.Create first example project
====
[source, shell]
----
include::examples/ch2_seldon_examples/setup_example.sh[tags=generate_kf_app]
----
====


[NOTE]
====
If you haven't added the scripts directory to your path for easy access you will to use the full path to your Kubeflow install.
====

[NOTE]
====
This fails with `ERROR handle object: patching object from cluster: merging object with existing state: unable to recognize "/tmp/ksonnet-mergepatch923988339": no matches for kind "Application" in version "app.k8s.io/v1beta1" `... But always works on round 2
====

==== Setting up our components (H)

Kubeflow project's can have all kinds of different components.
Many common ones, like seldon, are automatically installed by kfctl automatically.
In addition to the default components set up we will configure a few extra pieces:

.Set up components
====
[source, shell]
----
include::examples/ch2_seldon_examples/setup_example.sh[tags=setup_components]
----
====


==== Clone `example-seldon` (H)


For this simple example we are going to follow along with an example you can find at https://github.com/kubeflow/example-seldon.

Though we will be loosely following this example, we will be updating the version of `seldon-core` and changing the
persistant volume claim (storage) from Google Cloud to local disk.  We will also be building the docker containers
and pushing them to our own local registry.

The example source can be cloned with git <<get_seldon_example_source>>.

.Clone Seldon Example
[[get_seldon_example_source]]
====
[source, shell]
----
include::examples/ch2/setup_example.sh[tags=cloneSeldonExample]
----
====


==== Creating a Persistent Volume (T)

Why do we need a persistent volume? TODO

The original example used an https://kubernetes.io/docs/concepts/storage/volumes/#nfs[Network File System (NFS) Mount].
  While this is an ideal way to share information between systems in production, it is a bit complicated to set up "locally"
  (read `minikube` or `microk8s`).  We have updated the example to use

While the user is encouraged to https://kubernetes.io/docs/concepts/storage/persistent-volumes/[learn more] about the various options of `PersistentVolumes` it will be sufficient for
this example to follow the steps below to simply create one.

To create the `PersistantVolume` and `PersistantVolumeClaim`:

```bash
cd $KF_EXAMPLES_DIR
include::examples/ch2_seldon_examples/setup_example.sh[tags=createPV]
```

The first command will create a `PersistentVolume`, which is an allocation of storage space from an administrator which
independent of pod lifecycles.  The second command is for a `PersistentVolumeClaim` which is a request by a user for a
piece of the storace allotted in the `PersistentVolume` which can then be mounted to a pod.  We must have a `PeristentVolume`
from which a `PersistentVolumeClaim` can be made.

Also- notice we added the flag `-n kubeflow`

If that whole paragraph feels like I'm talking in circles- that's OK for now. Here is the important thing you need to
understand:

IMPORTANT: The `PeristentVolumeClaim` is where the trained model is stored, and then loaded for serving.


=== Training and deploying a Model

note intro

==== Building a Training Image (T)


Let's head back to the
`example-seldon/models/sk_mnist/train` directory.  In this directory you'll see a Dockerfile, and `create_model.py`.
What `create_model.py` does is download the MNIST dataset, train a RandomForrestClassifier on the data within, and save
trained model to `/data/sk.pkl`, which is our `PersistentVolumeClaim`.   The Dockerfile just installs `requirements.txt`
and waits for `/data` to be mounted before running `create_model.py`.


==== Training and Monitoring Training Progress (T)

Now, to actually train the model- let's go to the directory containing the `training-sk-mnist-workflow.yaml` workflow.  We use `argo` to submit this workflow in the `kubeflow` namespace.

```
cd ~/example-seldon/workflows
argo submit training-sk-mnist-workflow.yaml -n kubeflow
```

To see model training progress (there is a UI, but for simple "Hello World" we're skipping it for now).
```bash
 kubectl get pods -n kubeflow | grep sk-train
```

Should yeild something like this:
```
kubeflow-sk-train-wnbgj-1046465934                        0/1     Completed   0          5m11s
sk-train-kmpcr                                            0/1     Completed   0          4m31s
```

==== Building and Deploying Serving Images

note why, how, expected results

```bash
argo submit serving-sk-mnist-workflow.yaml -n kubeflow -p deploy-model=true
```

TIP: `-p deploy-model=true` is a feature of that particular workflow, and not nessicary for all kubeflow serving.


```bash
kubectl get pods -n kubeflow | grep sk-deploy
```

==== Test Query

note why, how, expected results

```bash
$ kubectl get services -n kubeflow | grep ambassador
ambassador                                           NodePort    10.152.183.112   <none>        80:30134/TCP        61m
ambassador-admin                                     ClusterIP   10.152.183.204   <none>        8877/TCP            61m
```

The model will be available at http://<MACHINE_IP>:<AMBASSADOR_PORT>/seldon/<DEPLOYMENT_NAME>/api/v0.1/predictions

There is a python script available at (insert path, it's in atlas) for pulling an image from the MNIST dataset, turning
into a vector, displaying the image, and sending to to the model.  The model returns a json of the 10 digits and the
probabiltiy that submitted image is what digit.

http://<MACHINE_IP>:<GRAFANA_PORT>/dashboard/db/prediction-analytics?refresh=5s&orgId=1
uid admin, pwd password


Load test:
```bash
kubectl label nodes $(kubectl get nodes -o jsonpath='{.items[0].metadata.name}') role=locust

helm install seldon-core-loadtesting --name loadtest  \
    --namespace kubeflow \
    --repo https://storage.googleapis.com/seldon-charts \
    --set locust.script=mnist_rest_locust.py \
    --set locust.host=http://mnist-classifier:8000 \
    --set oauth.enabled=false \
    --set oauth.key=oauth-key \
    --set oauth.secret=oauth-secret \
    --set locust.hatchRate=1 \
    --set locust.clients=1 \
    --set loadtest.sendFeedback=1 \
    --set locust.minWait=0 \
    --set locust.maxWait=0 \
    --set replicaCount=1 \
    --set data.size=784
```


```
{'data': {'names': ['class:0',
                    'class:1',
                    'class:2',
                    'class:3',
                    'class:4',
                    'class:5',
                    'class:6',
                    'class:7',
                    'class:8',
                    'class:9'],
          'ndarray':[[0.03333333333333333,
                      0.26666666666666666,
                      0.03333333333333333,
                      0.13333333333333333, ## It was actually this
                      0.1,
                      0.06666666666666667,
                      0.1,
                      0.26666666666666666,
                      0.0,
                      0.0]]},
 'meta': {'puid': 'tb02ff58vcinl82jmkkoe80u4r', 'routing': {}, 'tags': {}}}
```

==== Things to come

Preview of chapter X

=== Conclusion

We hope that your first adventure into Kubeflow has been a success.
In the next chapter we are going to introduce the case studies that we will develop
throughout the rest of the book.

//
//== Training and Serving a Simple Model with `sklearn`
//
//In this chapter we're going to do a "Hello World" of Kubeflow.  Kubeflow is a large and complicated system, and we're
// not going to turn every knob, set every dial, or utilize every feature.  What we are going to do is present a "simple"
// getting started-first pass attempt to achieve three things:
//
//- Train a simple `sklearn` model
//- Serve that model using `seldon-core`
//- Query that model to make sure it is (technically) functional .
//
//The first question the reader might ask is, "Why `sklearn`?  That's not Tensorflow"
//Well, maybe- but it's also one of
//the most widely used machine learning packages today, and while there is plenty of documentation about deploying
//fancy-pants deep learning models, there isn't much available for the old workhorse `sklearn`.  We are following an
//example that uses sklearn but was focused on `TensorFlow` so if you feel so inclined- you are welcome to change over to
//that.
//
//The second question the reader might ask is, "What do you mean by "technically" functional?"  In this example we're
//going to read handwritten digits via `RandomForrestClassifier` which is really the wrong tool for the job and does not
//perform well.  We really just care that we have a model, we can send it a digit (or at least a matrix repreesntation of
//digit), and it will respond with the digits zero through nine, and the models "guess" at how likely it is to be each of
//digits.
//
//The third question the reader might ask is, "Why are we using `seldon-core`? Flask is much easier." I would agree that
//_every thing in this entire book_ might seem like an unnessicarily long and overly complex approach to seemingly simple
//problems.  The reader should only be here if they are attempting to scale simple laptop problems to robust production
//systems.  The underlying idea here is that deploying to production is complex, data scientists do a poor job of handling
//complex things, and we would rather do hard work once to keep the data scientists away from the complexity because we
//know what we're doing.
//
//This chapter will consist of the following sections:
//
//- Cloning and modifying `example-seldon`
//- Training and observing the training of the model
//- Serving the model and checking that it "works"
//
//=== Hello World
//
//
//
//
//If you create your own persistent volume clain and name it something else, simply enter that here.
//
//Secondly, we are going to update the image to use. A few lines up (appoximately line 87) you will see:
//
//```
//{{workflow.parameters.docker-user}}/skmnistclassifier_trainer:{{workflow.parameters.version}}
//```
//
//We want to change that to
//
//```
//localhost:32000/skmnistclassifier_trainer:latest
//```
//
//The observant reader will have noticed that we didn't _need_ to change that, instead we could have simply set `docker-user`
//and `version` however, we want the reader to see where this is being set with the idea they will be writing their own someday
//soon.
//
//Next we're going to edit `$EXAMPLE_SELDON_CLONE_DIR/workflows/serving-sk-mnist-workflow.yaml`.
//The steps are similar. `claimName: "nfs-1"` is a few lines from the bottom (not the dead last line as previously).
//The `image` is a few lines above that.
//
//=== Building Images, Training, and Serving
//
//
//====== Serving
//
//
//
//== Querying the Model
//
//
//== Conclusion
//
