[[simple_training_ch]]
== New stuff starts here- (Trevo's refactor)

== Hello Kubeflow

Introduction

=== Setup

We assume the user has the following already installed:

* Kubernetes, v1.13.3
* Helm, v2.13.0
* KSonnet, v0.13.1
* Argo, v2.2.1
* Kubeflow, v0.4.1

If you don't have all of these requirements already installed, please install them first.

==== Creating Our First Kubeflow Project

Before we get started, we will need to make a Kubeflow project for us to work in.
The script `kfctl.sh`, in the scripts directory of your Kubeflow installation,
allows you to create projects. We'll start with an example project in <<create_example_project>>.

[[create_example_project]]
.Create first example project
====
[source, shell]
----
include::examples/ch2_seldon_examples/setup_example.sh[tags=generate_kf_app]
----
====


[NOTE]
====
If you haven't added the scripts directory to your path for easy access you will to use the full path to your Kubeflow install.
====

[EDITOR]
====
This fails with `ERROR handle object: patching object from cluster: merging object with existing state: unable to recognize "/tmp/ksonnet-mergepatch923988339": no matches for kind "Application" in version "app.k8s.io/v1beta1" `... But always works on round 2
====

==== Setting Up Components

Kubeflow project's can have all kinds of different components.
Many common ones, like seldon, are automatically installed by kfctl automatically.
In addition to the default components set up we will configure a few extra pieces:

.Set up components
====
[source, shell]
----
include::examples/ch2_seldon_examples/setup_example.sh[tags=setup_components]
----
====


==== Cloning the Example


For this simple example we are going to follow along with an example you can find at https://github.com/kubeflow/example-seldon.

The example source can be cloned with git <<get_seldon_example_source>>.

.Clone Seldon Example
[[get_seldon_example_source]]
====
[source, shell]
----
include::examples/ch2/setup_example.sh[tags=cloneSeldonExample]
----
====


==== Creating a Persistent Volume

Kubeflow is a system that decouples the training and serving of models. In other words, models are trained in one
container and served in a different container.  The actual trained model must be shared from the container that trains
the model to the container that is serving it.  There are several ways to achieve this goal, but the easiest is a
`PersistentVolume`.  A `PersistentVolume` is an allocation fo disk space that survives beyond a pods lifecycle.

The original example used an https://kubernetes.io/docs/concepts/storage/volumes/#nfs[Network File System (NFS) Mount].
  While this is an ideal way to share information between systems in production, it is a bit complicated to set up "locally"
  (read `minikube` or `microk8s`).  We have updated the example to use a `local` mount. Please note, if you using GCP to
  do this example, you can skip this step.  This step is only to create a local `PersistentVolume` and
  `PersistentVolumeClaim`

While the user is encouraged to https://kubernetes.io/docs/concepts/storage/persistent-volumes/[learn more] about the various options of `PersistentVolumes` it will be sufficient for
this example to follow the steps below to simply create one.

To create the `PersistantVolume` and `PersistantVolumeClaim` see <<create_pv>>.


.Create Persistent Volume Example
[[create_pv]]
====
[source, shell]
----
include::examples/ch2_seldon_examples/setup_example.sh[tags=createPV]
----
====



The first command will create a `PersistentVolume`, which is an allocation of storage space from an administrator which
independent of pod lifecycles.  The second command is for a `PersistentVolumeClaim` which is a request by a user for a
piece of the storace allotted in the `PersistentVolume` which can then be mounted to a pod.  We must have a `PeristentVolume`
from which a `PersistentVolumeClaim` can be made.

If that whole paragraph feels like I'm talking in circles- that's OK for now. Here is the important thing you need to
understand:

IMPORTANT: The `PeristentVolumeClaim` is where the trained model is stored, and then loaded for serving.


=== Training and Deploying a Model

In traditional "machine learning" texts, this phase is the one that most of the book is dedicated to, with a few simple
examples on deployment, and very little treatment on model management.  In this text however, we assume chosing what
algorithm/model to use is either something the practitioner is very familiar with or not the practitioners job, and so
they don't really care.  We will point out where the reader may see the training code as we drive by and not dive into it at all.

==== Building a Training Image

Let's head back to the
`example-seldon/models/sk_mnist/train` directory.  In this directory you'll see a Dockerfile, and `create_model.py`.
What `create_model.py` does is download the MNIST dataset, train a RandomForrestClassifier on the data within, and save
trained model to `/data/sk.pkl`, which is our `PersistentVolumeClaim`.   The Dockerfile just installs `requirements.txt`
and waits for `/data` to be mounted before running `create_model.py`.

While this process is intersting, and worth taking a look at, it is not required for this simple example.  We only point
it out to show that there is no magic happening here, and to give the reader a preview of things to come.

==== Training and Monitoring Training Progress (TODO)

Now, to actually train the model- let's go to the directory containing the `training-sk-mnist-workflow.yaml` workflow.
We use `argo` to submit this workflow in the `kubeflow` namespace.

```
cd ~/example-seldon/workflows
argo submit training-sk-mnist-workflow.yaml -n kubeflow
```

To see model training progress (there is a UI, but for simple "Hello World" we're skipping it for now).
```bash
 kubectl get pods -n kubeflow | grep sk-train
```

Should yeild something like this:
```
kubeflow-sk-train-wnbgj-1046465934                        0/1     Completed   0          5m11s
sk-train-kmpcr                                            0/1     Completed   0          4m31s
```

==== Building and Deploying Serving Images

note why, how, expected results

```bash
argo submit serving-sk-mnist-workflow.yaml -n kubeflow -p deploy-model=true
```

TIP: `-p deploy-model=true` is a feature of this particular workflow, but not nessicary for all kubeflow serving.


```bash
kubectl get pods -n kubeflow | grep sk-deploy
```

==== Test Query

note why, how, expected results

```bash
$ kubectl get services -n kubeflow | grep ambassador
ambassador                                           NodePort    10.152.183.112   <none>        80:30134/TCP        61m
ambassador-admin                                     ClusterIP   10.152.183.204   <none>        8877/TCP            61m
```

The model will be available at http://<MACHINE_IP>:<AMBASSADOR_PORT>/seldon/<DEPLOYMENT_NAME>/api/v0.1/predictions

There is a python script available at (insert path, it's in atlas) for pulling an image from the MNIST dataset, turning
into a vector, displaying the image, and sending to to the model.  The model returns a json of the 10 digits and the
probabiltiy that submitted image is what digit.

http://<MACHINE_IP>:<GRAFANA_PORT>/dashboard/db/prediction-analytics?refresh=5s&orgId=1
uid admin, pwd password


Load test:
```bash
kubectl label nodes $(kubectl get nodes -o jsonpath='{.items[0].metadata.name}') role=locust

helm install seldon-core-loadtesting --name loadtest  \
    --namespace kubeflow \
    --repo https://storage.googleapis.com/seldon-charts \
    --set locust.script=mnist_rest_locust.py \
    --set locust.host=http://mnist-classifier:8000 \
    --set oauth.enabled=false \
    --set oauth.key=oauth-key \
    --set oauth.secret=oauth-secret \
    --set locust.hatchRate=1 \
    --set locust.clients=1 \
    --set loadtest.sendFeedback=1 \
    --set locust.minWait=0 \
    --set locust.maxWait=0 \
    --set replicaCount=1 \
    --set data.size=784
```


```
{'data': {'names': ['class:0',
                    'class:1',
                    'class:2',
                    'class:3',
                    'class:4',
                    'class:5',
                    'class:6',
                    'class:7',
                    'class:8',
                    'class:9'],
          'ndarray':[[0.03333333333333333,
                      0.26666666666666666,
                      0.03333333333333333,
                      0.13333333333333333, ## It was actually this
                      0.1,
                      0.06666666666666667,
                      0.1,
                      0.26666666666666666,
                      0.0,
                      0.0]]},
 'meta': {'puid': 'tb02ff58vcinl82jmkkoe80u4r', 'routing': {}, 'tags': {}}}
```

==== Things to come

Preview of chapter X

=== Conclusion

We hope that your first adventure into Kubeflow has been a success.
In the next chapter we are going to introduce the case studies that we will develop
throughout the rest of the book.

//
//== Training and Serving a Simple Model with `sklearn`
//
//In this chapter we're going to do a "Hello World" of Kubeflow.  Kubeflow is a large and complicated system, and we're
// not going to turn every knob, set every dial, or utilize every feature.  What we are going to do is present a "simple"
// getting started-first pass attempt to achieve three things:
//
//- Train a simple `sklearn` model
//- Serve that model using `seldon-core`
//- Query that model to make sure it is (technically) functional .
//
//The first question the reader might ask is, "Why `sklearn`?  That's not Tensorflow"
//Well, maybe- but it's also one of
//the most widely used machine learning packages today, and while there is plenty of documentation about deploying
//fancy-pants deep learning models, there isn't much available for the old workhorse `sklearn`.  We are following an
//example that uses sklearn but was focused on `TensorFlow` so if you feel so inclined- you are welcome to change over to
//that.
//
//The second question the reader might ask is, "What do you mean by "technically" functional?"  In this example we're
//going to read handwritten digits via `RandomForrestClassifier` which is really the wrong tool for the job and does not
//perform well.  We really just care that we have a model, we can send it a digit (or at least a matrix repreesntation of
//digit), and it will respond with the digits zero through nine, and the models "guess" at how likely it is to be each of
//digits.
//
//The third question the reader might ask is, "Why are we using `seldon-core`? Flask is much easier." I would agree that
//_every thing in this entire book_ might seem like an unnessicarily long and overly complex approach to seemingly simple
//problems.  The reader should only be here if they are attempting to scale simple laptop problems to robust production
//systems.  The underlying idea here is that deploying to production is complex, data scientists do a poor job of handling
//complex things, and we would rather do hard work once to keep the data scientists away from the complexity because we
//know what we're doing.
//
//This chapter will consist of the following sections:
//
//- Cloning and modifying `example-seldon`
//- Training and observing the training of the model
//- Serving the model and checking that it "works"
//
//=== Hello World
//
//
//
//
//If you create your own persistent volume clain and name it something else, simply enter that here.
//
//Secondly, we are going to update the image to use. A few lines up (appoximately line 87) you will see:
//
//```
//{{workflow.parameters.docker-user}}/skmnistclassifier_trainer:{{workflow.parameters.version}}
//```
//
//We want to change that to
//
//```
//localhost:32000/skmnistclassifier_trainer:latest
//```
//
//The observant reader will have noticed that we didn't _need_ to change that, instead we could have simply set `docker-user`
//and `version` however, we want the reader to see where this is being set with the idea they will be writing their own someday
//soon.
//
//Next we're going to edit `$EXAMPLE_SELDON_CLONE_DIR/workflows/serving-sk-mnist-workflow.yaml`.
//The steps are similar. `claimName: "nfs-1"` is a few lines from the bottom (not the dead last line as previously).
//The `image` is a few lines above that.
//
//=== Building Images, Training, and Serving
//
//
//====== Serving
//
//
//
//== Querying the Model
//
//
//== Conclusion
//
