[[simple_training_ch]]
== Training and Serving a Simple Model with `sklearn`

In this chapter we're going to do a "Hello World" of Kubeflow.  Kubeflow is a large and complicated system, and we're
 not going to turn every knob, set every dial, or utilize every feature.  What we are going to do is present a "simple"
 getting started-first pass attempt to achieve three things:

- Train a simple `sklearn` model
- Serve that model using `seldon-core`
- Query that model to make sure it is (technically) functional .

The first question the reader might ask is, "Why `sklearn`?  That's boring stuff for babies." Well, maybe- but it's also one of
the most widely used machine learning packages today, and while there is plenty of documentation about deploying
fancy-pants deep learning models, there isn't much available for the old workhorse `sklearn`.  We are following an
example that uses sklearn but was focused on `TensorFlow` so if you feel so inclined- you are welcome to change over to
that.

The second question the reader might ask is, "What do you mean by "technically" functional?"  In this example we're
going to read handwritten digits via `RandomForrestClassifier` which is really the wrong tool for the job and does not
perform well.  We really just care that we have a model, we can send it a digit (or at least a matrix repreesntation of
digit), and it will respond with the digits zero through nine, and the models "guess" at how likely it is to be each of
digits.

The third question the reader might ask is, "Why are we using `seldon-core`? Flask is much easier." I would agree that
_every thing in this entire book_ might seem like an unnessicarily long and overly complex approach to seemingly simple
problems.  The reader should only be here if they are attempting to scale simple laptop problems to robust production
systems.  The underlying idea here is that deploying to production is complex, data scientists do a poor job of handling
complex things, and we would rather do hard work once to keep the data scientists away from the complexity because we
know what we're doing.

This chapter will consist of the following sections:

- Installing `microk8s` on `multipass`, along with supporting tools (to be moved to appendix?)
- Cloning and modifying `example-seldon`
- Training and observing the training of the model
- Serving the model and checking that it "works"

=== Install `microk8s` on `multipass` with support tooling

Multipass (https://github.com/CanonicalLtd/multipass) is a system for orchestrating virtual machines in Linux or OSX.
If you have a Linux machine you could simply install `kubectl` and friends on the system, however when learning, I have
found it's usually nice to have a 'throw away' system to learn on.  Multipass is also very useful for OSX because trying
do anything besides playing stuff on iTunes and writing blogs on OSX is an aweful experience I wouldn't wish on my worst
enemy.

==== Installing `multipass`

====== On OSX

To install `multipass` on OSX, go to https://github.com/CanonicalLtd/multipass/releases and download `*-Darwin.pkg` from
the latest release.  Follow the instructions in the installer.

====== On Linux

To install `multipass` in a Linux environment using `snap` run the following at the command line interface:

```
sudo snap install multipass --beta --classic
```

===== Create Machine and Setup `microk8s`

First we must create a new virtual machine.

```
multipass launch bionic -n kubeflow -m 8G -d 40G -c 4
```

This will create a VM based on `bionic-beaver` also known as Ubuntu 18.04.  We are naming the machine `kubeflow` and
alloting it 8 GB of memory, 40 GB of disk space and 4 cpu cores. You may need to reduce those amount on smaller systems.

Next we install `microk8s` on our virtual machine.

```
multipass exec kubeflow -- sudo snap install microk8s --classic
```

Now lets enter our virtual machines shell and set some settings.

```
multipass shell kubeflow
```

The following will enable the `microk8s.docker` registry and DNS dashboard, and create aliases so `kubectl` and `docker`
 will use `microk8s.kubectl` and `microk8s.docker` respectively.  We update the `DOCKER_HOST` environmental variable
 because the `microk8s.docker` version behaves slightly differently from the standard `docker`. Finally, you'll notice we update `iptables` to accept
 incoming connections, we do this so we can see the web-uis of various services we are running.

```
microk8s.enable dns dashboard # turns off FORAWARD ACCEPT
sudo iptables -P FORWARD ACCEPT
sudo snap alias microk8s.kubectl kubectl

### Faking Docker
microk8s.enable registry
sudo snap alias microk8s.docker docker
export DOCKER_HOST="unix:///var/snap/microk8s/current/docker.sock"
sudo ln -s /var/snap/microk8s/current/docker.sock /var/run/docker.sock
sudo ln -s /var/snap/microk8s/common/var/lib/docker /var/lib/docker
```

==== Installing all of our Friends

Kubernetes has _lots_ of friends.  Here we present a simple recipe for installing them all.  The user is encouraged to
go research what each does on their own time.

```
### K8s Tools
git clone https://github.com/canonical-labs/kubernetes-tools
sudo kubernetes-tools/setup-microk8s.sh

### Kubeflow Tools
git clone https://github.com/canonical-labs/kubeflow-tools
kubeflow-tools/install-kubeflow.sh

### KSonnet
wget https://github.com/ksonnet/ksonnet/releases/download/v0.13.1/ks_0.13.1_linux_amd64.tar.gz
tar -xzf *gz
rm *gz
sudo cp ks*/ks /usr/local/bin

### Argo
udo curl -sSL -o /usr/local/bin/argo https://github.com/argoproj/argo/releases/download/v2.2.1/argo-linux-amd64
sudo chmod +x /usr/local/bin/argo
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo/v2.2.1/manifests/install.yaml

### Helm and Tiller
sudo snap install helm --classic
kubectl -n kube-system create sa tiller
kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
helm init --service-account tiller
kubectl rollout status deploy/tiller-deploy -n kube-system
```


=== Hello World

===== Clone `example-seldon`

TIP: Will need to be updated to factor out `ks`

For this simple example we are going to follow along with an example you can find at https://github.com/kubeflow/example-seldon.

Though we will be loosely following this example, we will be updating the version of `seldon-core` and changing the
persistant volume claim (storage) from Google Cloud to local disk.  We will also be building the docker containers
and pushing them to our own local registry.

While in the shell of the virtual machine- run the following command to clone the example source code:

```
git clone https://github.com/kubeflow/example-seldon
```

I'm not going to say too much about this bc it's getting factored out, but here is a recipe.
```
cd ~/my_kubeflow/ks_app  # created by kubeflow-tools/install-kubeflow.sh
ks pkg install kubeflow/seldon
ks pkg install kubeflow/argo

ks generate seldon seldon
ks generate argo kubeflow-argo

ks apply default -c seldon
ks apply default -c kubeflow-argo
```
===== Creating a Persistent Volume

The first place we are going to diverge from the provided example is instead of using Google NFS for our storage, we are
goign to create a Persistant Volume Claim on local disks.  See https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/
 for more information.

To create the `PersistantVolume` and `PersistantVolumeClaim`:

```bash
kubectl create -f https://k8s.io/examples/pods/storage/pv-volume.yaml -n kubeflow
kubectl create -f https://k8s.io/examples/pods/storage/pv-claim.yaml -n kubeflow
```

This will download the `.yaml` file examples which will create a persistent volume named `task-pv-volume` and a
persistent volume claim against `task-pv-volume` named `task-pv-claim`. We will see in the next section that this
persistent disk space is only used to save and load the model, however we could also load training data in this way
(MNIST data is downloaded in `create_model.py`). The reader is encouraged to learn more on their own about `PersistentVolume`
and `PersistentVolumeClaim` s.

===== Updating YAMLs

We are going to be updating two `yaml` files. `$EXAMPLE_SELDON_CLONE_DIR/workflows/training-sk-mnist-workflow.yaml`
and `$EXAMPLE_SELDON_CLONE_DIR/workflows/serving-sk-mnist-workflow.yaml`.  We will need to edit these in the virtual
machine, but it might be easier to look at them here:

- https://github.com/kubeflow/example-seldon/blob/master/workflows/training-sk-mnist-workflow.yaml
- https://github.com/kubeflow/example-seldon/blob/master/workflows/serving-sk-mnist-workflow.yaml

We want to accomplish three things.  First- are going to update them both to use our newly created `PersistentVolumeClaim`
instead of the Google based on originally called for in the example.  Next we are going to update them both to point to
local Docker images we will be creating in the next step. Finally, we will update the serving `yaml` to use a newer version
`seldon-core`.

In the VM shell, edit `$EXAMPLE_SELDON_CLONE_DIR/workflows/training-sk-mnist-workflow.yaml`.  The first edit we will make
is the last line you will see:

```
 claimName: "nfs-1"
```

We want to change that to

```
 claimName: task-pv-claim
```

If you create your own persistent volume clain and name it something else, simply enter that here.

Secondly, we are going to update the image to use. A few lines up (appoximately line 87) you will see:

```
{{workflow.parameters.docker-user}}/skmnistclassifier_trainer:{{workflow.parameters.version}}
```

We want to change that to

```
localhost:32000/skmnistclassifier_trainer:latest
```

The observant reader will have noticed that we didn't _need_ to change that, instead we could have simply set `docker-user`
and `version` however, we want the reader to see where this is being set with the idea they will be writing their own someday
soon.

Next we're going to edit `$EXAMPLE_SELDON_CLONE_DIR/workflows/serving-sk-mnist-workflow.yaml`.
The steps are similar. `claimName: "nfs-1"` is a few lines from the bottom (not the dead last line as previously).
The `image` is a few lines above that.

=== Building Images, Training, and Serving

====== Building and Training

It might seem a bit strange that in a "Hello World" of a system that is all about building, training and serving machine
learning models, that this is just a short section at the end, but here we are.  Kubeflow is about setting up pipelines
to make it easy for data scientists to build, train, and serve models- and it is up to you the reader to make that
process easy for them (unless you're a data scientist who is just reading this for fun).

In the `kubeflow` virutal machine (or wherever you've been running all of this stuff) let's head back to the
`example-seldon/models/sk_mnist/train` directory.  In this directory you'll see a Dockerfile, and `create_model.py`.
What `create_model.py` does is download the MNIST dataset, train a RandomForrestClassifier on the data within, and save
trained model to `/data/sk.pkl`, which is our `PersistentVolumeClaim`.   The Dockerfile just installs `requirements.txt`
and waits for `/data` to be mounted before running `create_model.py`.

We're going to push the model to our local Docker repository, which in `microk8s` is available on port 32000.

```
cd ~/example-seldon/models/sk_mnist/train

docker build . -t localhost:32000/skmnistclassifier_trainer:latest
docker push localhost:32000/skmnistclassifier_trainer:latest
```


Now, to actually train the model- we go back to the directory containing the `training-sk-mnist-workflow.yaml` workflow
we updated earlier.  We use `argo` to submit this workflow in the `kubeflow` namespace.

```
cd ~/example-seldon/workflows
argo submit training-sk-mnist-workflow.yaml -n kubeflow
```

Now we can "watch" our model being trained (or at least the argo workflow).  Get the IP address of the multipass virtual
 machine by typing this into a terminal on your host machine.

```
multipass list
```

Then in the kubeflow machine shell

```
kubectl get pods -n kubeflow | grep ambassador
```

And get the port ambassador is being served on.

Finally serf to http://<your-vm-ip>:<ambassador-port>/argo/  Training time will vary.

====== Serving

Once training is done, we want to serve the model using Seldon Core. To do that we have one last install we must do-
`s2i`, otherwise known as `source2image` which wraps our model serving code into an image for serving.  To install `s2i`
follow this recipe in the virtual machine.

```bash
sudo apt install make
sudo snap install go --classic
wget https://github.com/openshift/source-to-image/releases/download/v1.1.13/source-to-image-v1.1.13-b54d75d3-linux-amd64.tar.gz
tar -xzf source-to-image-v1.1.13-b54d75d3-linux-amd64.tar.gz
sudo cp ~/s2i /usr/local/bin
```

Now we can use `s2i` to create the serving image in `example-seldon/models/sk_mnist/runtime`.  We are using the
`seldon-core` python2  base image.  Obviously you will want to use python3 as python2 has been deprecated for sometime
now.  We then push that image into our local Docker repository, and launch the job with `argo`

```bash
cd example-seldon/models/sk_mnist/runtime
s2i build . seldonio/seldon-core-s2i-python2:0.4 localhost:32000/skmnistclassifier_runtime:0.2
docker push localhost:32000/skmnistclassifier_runtime:0.2
argo submit serving-sk-mnist-workflow.yaml -n kubeflow -p deploy-model=true
```

TIP: `-p deploy-model=true` is a feature of that particular workflow, and not nessicary for all kubeflow serving.

====== Querying the Model

The model will be available at http://<AMBASSADOR_IP>:<AMBASSADOR_PORT>/seldon/<DEPLOYMENT_NAME>/api/v0.1/predictions

There is a python script available at (insert path, it's in atlas) for pulling an image from the MNIST dataset, turning
into a vector, displaying the image, and sending to to the model.  The model returns a json of the 10 digits and the 
probabiltiy that submitted image is what digit. 