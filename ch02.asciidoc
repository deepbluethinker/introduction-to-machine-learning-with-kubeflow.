[[simple_training_ch]]
== Training and Serving a Simple Model with `sklearn`

In this chapter we're going to do a "Hello World" of Kubeflow.  Kubeflow is a large and complicated system, and we're
 not going to turn every knob, set every dial, or utilize every feature.  What we are going to do is present a "simple"
 getting started-first pass attempt to achieve three things:

- Train a simple `sklearn` model
- Serve that model using `seldon-core`
- Query that model to make sure it is (technically) functional .

The first question the reader might ask is, "Why `sklearn`?  That's not Tensorflow"
Well, maybe- but it's also one of
the most widely used machine learning packages today, and while there is plenty of documentation about deploying
fancy-pants deep learning models, there isn't much available for the old workhorse `sklearn`.  We are following an
example that uses sklearn but was focused on `TensorFlow` so if you feel so inclined- you are welcome to change over to
that.

The second question the reader might ask is, "What do you mean by "technically" functional?"  In this example we're
going to read handwritten digits via `RandomForrestClassifier` which is really the wrong tool for the job and does not
perform well.  We really just care that we have a model, we can send it a digit (or at least a matrix repreesntation of
digit), and it will respond with the digits zero through nine, and the models "guess" at how likely it is to be each of
digits.

The third question the reader might ask is, "Why are we using `seldon-core`? Flask is much easier." I would agree that
_every thing in this entire book_ might seem like an unnessicarily long and overly complex approach to seemingly simple
problems.  The reader should only be here if they are attempting to scale simple laptop problems to robust production
systems.  The underlying idea here is that deploying to production is complex, data scientists do a poor job of handling
complex things, and we would rather do hard work once to keep the data scientists away from the complexity because we
know what we're doing.

This chapter will consist of the following sections:

- Cloning and modifying `example-seldon`
- Training and observing the training of the model
- Serving the model and checking that it "works"

=== Hello World

==== Creating a Kubeflow project

Before we get started, we will need to make a Kubeflow project for us to work in.
The script `kfctl.sh`, in the scripts directory of your Kubeflow installation,
allows you to create projects. We'll start with an example project in <<create_example_project>>.

[[create_example_project]]
.Create first example project
====
[source, shell]
----
include::examples/ch2_seldon_examples/setup_example.sh[tags=generate_kf_app]
----
====


[NOTE]
====
If you haven't added the scripts directory to your path for easy access you will to use the full path to your Kubeflow install.
====

==== Setting up our components

Kubeflow project's can have all kinds of different components.
Many common ones, like seldon, are automatically installed by kfctl automatically.
In addition to the default components set up we will configure a few extra pieces:

.Set up components
====
[source, shell]
----
include::examples/ch2_seldon_examples/setup_example.sh[tags=setup_components]
----
====


==== Clone `example-seldon`


For this simple example we are going to follow along with an example you can find at https://github.com/kubeflow/example-seldon.

Though we will be loosely following this example, we will be updating the version of `seldon-core` and changing the
persistant volume claim (storage) from Google Cloud to local disk.  We will also be building the docker containers
and pushing them to our own local registry.

The example source can be cloned with git <<get_seldon_example_source>>.

.Clone Seldon Example
[[get_seldon_example_source]]
====
[source, shell]
----
include::examples/ch2/setup_example.sh[tags=clone]
----
====



==== Creating a Persistent Volume

The first place we are going to diverge from the provided example is instead of using Google NFS for our storage, we are
goign to create a Persistant Volume Claim on local disks.  See https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/
 for more information.

To create the `PersistantVolume` and `PersistantVolumeClaim`:

```bash
kubectl create -f https://k8s.io/examples/pods/storage/pv-volume.yaml -n kubeflow
kubectl create -f https://k8s.io/examples/pods/storage/pv-claim.yaml -n kubeflow
```

This will download the `.yaml` file examples which will create a persistent volume named `task-pv-volume` and a
persistent volume claim against `task-pv-volume` named `task-pv-claim`. We will see in the next section that this
persistent disk space is only used to save and load the model, however we could also load training data in this way
(MNIST data is downloaded in `create_model.py`). The reader is encouraged to learn more on their own about `PersistentVolume`
and `PersistentVolumeClaim` s.

==== Updating YAMLs

We are going to be updating two `yaml` files. `$EXAMPLE_SELDON_CLONE_DIR/workflows/training-sk-mnist-workflow.yaml`
and `$EXAMPLE_SELDON_CLONE_DIR/workflows/serving-sk-mnist-workflow.yaml`.  We will need to edit these in the virtual
machine, but it might be easier to look at them here:

- https://github.com/kubeflow/example-seldon/blob/master/workflows/training-sk-mnist-workflow.yaml
- https://github.com/kubeflow/example-seldon/blob/master/workflows/serving-sk-mnist-workflow.yaml

We want to accomplish three things.  First- are going to update them both to use our newly created `PersistentVolumeClaim`
instead of the Google based on originally called for in the example.  Next we are going to update them both to point to
local Docker images we will be creating in the next step. Finally, we will update the serving `yaml` to use a newer version
`seldon-core`.

In the VM shell, edit `$EXAMPLE_SELDON_CLONE_DIR/workflows/training-sk-mnist-workflow.yaml`.  The first edit we will make
is the last line you will see:

```
 claimName: "nfs-1"
```

We want to change that to

```
 claimName: task-pv-claim
```

If you create your own persistent volume clain and name it something else, simply enter that here.

Secondly, we are going to update the image to use. A few lines up (appoximately line 87) you will see:

```
{{workflow.parameters.docker-user}}/skmnistclassifier_trainer:{{workflow.parameters.version}}
```

We want to change that to

```
localhost:32000/skmnistclassifier_trainer:latest
```

The observant reader will have noticed that we didn't _need_ to change that, instead we could have simply set `docker-user`
and `version` however, we want the reader to see where this is being set with the idea they will be writing their own someday
soon.

Next we're going to edit `$EXAMPLE_SELDON_CLONE_DIR/workflows/serving-sk-mnist-workflow.yaml`.
The steps are similar. `claimName: "nfs-1"` is a few lines from the bottom (not the dead last line as previously).
The `image` is a few lines above that.

=== Building Images, Training, and Serving

==== Building and Training

It might seem a bit strange that in a "Hello World" of a system that is all about building, training and serving machine
learning models, that this is just a short section at the end, but here we are.  Kubeflow is about setting up pipelines
to make it easy for data scientists to build, train, and serve models- and it is up to you the reader to make that
process easy for them (unless you're a data scientist who is just reading this for fun).

In the `kubeflow` virutal machine (or wherever you've been running all of this stuff) let's head back to the
`example-seldon/models/sk_mnist/train` directory.  In this directory you'll see a Dockerfile, and `create_model.py`.
What `create_model.py` does is download the MNIST dataset, train a RandomForrestClassifier on the data within, and save
trained model to `/data/sk.pkl`, which is our `PersistentVolumeClaim`.   The Dockerfile just installs `requirements.txt`
and waits for `/data` to be mounted before running `create_model.py`.

We're going to push the model to our local Docker repository, which in `microk8s` is available on port 32000.

```
cd ~/example-seldon/models/sk_mnist/train

docker build . -t localhost:32000/skmnistclassifier_trainer:latest
docker push localhost:32000/skmnistclassifier_trainer:latest
```


Now, to actually train the model- we go back to the directory containing the `training-sk-mnist-workflow.yaml` workflow
we updated earlier.  We use `argo` to submit this workflow in the `kubeflow` namespace.

```
cd ~/example-seldon/workflows
argo submit training-sk-mnist-workflow.yaml -n kubeflow
```

Now we can "watch" our model being trained (or at least the argo workflow).  Get the IP address of the multipass virtual
 machine by typing this into a terminal on your host machine.

```
multipass list
```

Then in the kubeflow machine shell

```
kubectl get pods -n kubeflow | grep ambassador
```

And get the port ambassador is being served on.

Finally serf to http://<your-vm-ip>:<ambassador-port>/argo/  Training time will vary.

====== Serving

Once training is done, we want to serve the model using Seldon Core. To do that we have one last install we must do-
`s2i`, otherwise known as `source2image` which wraps our model serving code into an image for serving.  To install `s2i`
follow this recipe in the virtual machine.

```bash
sudo apt install make
sudo snap install go --classic
wget https://github.com/openshift/source-to-image/releases/download/v1.1.13/source-to-image-v1.1.13-b54d75d3-linux-amd64.tar.gz
tar -xzf source-to-image-v1.1.13-b54d75d3-linux-amd64.tar.gz
sudo cp ~/s2i /usr/local/bin
```

Now we can use `s2i` to create the serving image in `example-seldon/models/sk_mnist/runtime`.  We are using the
`seldon-core` python2  base image.  Obviously you will want to use python3 as python2 has been deprecated for sometime
now.  We then push that image into our local Docker repository, and launch the job with `argo`

```bash
cd example-seldon/models/sk_mnist/runtime
s2i build . seldonio/seldon-core-s2i-python2:0.4 localhost:32000/skmnistclassifier_runtime:0.2
docker push localhost:32000/skmnistclassifier_runtime:0.2
argo submit serving-sk-mnist-workflow.yaml -n kubeflow -p deploy-model=true
```

TIP: `-p deploy-model=true` is a feature of that particular workflow, and not nessicary for all kubeflow serving.

== Querying the Model

The model will be available at http://<AMBASSADOR_IP>:<AMBASSADOR_PORT>/seldon/<DEPLOYMENT_NAME>/api/v0.1/predictions

There is a python script available at (insert path, it's in atlas) for pulling an image from the MNIST dataset, turning
into a vector, displaying the image, and sending to to the model.  The model returns a json of the 10 digits and the 
probabiltiy that submitted image is what digit. 

== Conclusion

We hope that your first adventure into Kubeflow has been a success.
In the next chapter we are going to introduce the case studies that we will develop
throughout the rest of the book.
