[[simple_training_ch]]
== Hello Kubeflow

Introduction

=== Setup

We assume the user has the following already installed:

* Kubernetes, v1.13.3
* Helm, v2.13.0
* KSonnet, v0.13.1
* Argo, v2.2.1
* Kubeflow, v0.4.1

If you don't have all of these requirements already installed, please install them first.

==== Creating Our First Kubeflow Project

Before we get started, we will need to make a Kubeflow project for us to work in.
The script `kfctl.sh`, in the scripts directory of your Kubeflow installation,
allows you to create projects. We'll start with an example project in <<create_example_project>>.

[[create_example_project]]
.Create first example project
====
[source, shell]
----
include::examples/ch2_seldon_examples/setup_example.sh[tags=generate_kf_app]
----
====


[NOTE]
====
If you haven't added the scripts directory to your path for easy access you will to use the full path to your Kubeflow install.
====

[EDITOR]
====
This fails with `ERROR handle object: patching object from cluster: merging object with existing state: unable to recognize "/tmp/ksonnet-mergepatch923988339": no matches for kind "Application" in version "app.k8s.io/v1beta1" `... But always works on round 2
====

==== Setting Up Components

Kubeflow project's can have all kinds of different components.
Many common ones, like seldon, are automatically installed by kfctl automatically.
In addition to the default components set up we will configure a few extra pieces:

.Set up components
====
[source, shell]
----
include::examples/ch2_seldon_examples/setup_example.sh[tags=setup_components]
----
====


==== Cloning the Example


For this simple example we are going to follow along with an example you can find at https://github.com/kubeflow/example-seldon.

The example source can be cloned with git <<get_seldon_example_source>>.

.Clone Seldon Example
[[get_seldon_example_source]]
====
[source, shell]
----
include::examples/ch2_seldon_examples/setup_example.sh[tags=cloneSeldonExample]
----
====


==== Creating a Persistent Volume

Kubeflow is a system that decouples the training and serving of models. In other words, models are trained in one
container and served in a different container.  The actual trained model must be shared from the container that trains
the model to the container that is serving it.  There are several ways to achieve this goal, but the easiest is a
`PersistentVolume`.  A `PersistentVolume` is an allocation fo disk space that survives beyond a pods lifecycle.

The original example used an https://kubernetes.io/docs/concepts/storage/volumes/#nfs[Network File System (NFS) Mount].
  While this is an ideal way to share information between systems in production, it is a bit complicated to set up "locally"
  (read `minikube` or `microk8s`).  We have updated the example to use a `local` mount. Please note, if you using GCP to
  do this example, you can skip this step.  This step is only to create a local `PersistentVolume` and
  `PersistentVolumeClaim`

While the user is encouraged to https://kubernetes.io/docs/concepts/storage/persistent-volumes/[learn more] about the various options of `PersistentVolumes` it will be sufficient for
this example to follow the steps below to simply create one.

To create the `PersistantVolume` and `PersistantVolumeClaim` see <<create_pv>>.


.Create Persistent Volume Example
[[create_pv]]
====
[source, shell]
----
include::examples/ch2_seldon_examples/setup_example.sh[tags=createPV]
----
====



The first command will create a `PersistentVolume`, which is an allocation of storage space from an administrator which
independent of pod lifecycles.  The second command is for a `PersistentVolumeClaim` which is a request by a user for a
piece of the storace allotted in the `PersistentVolume` which can then be mounted to a pod.  We must have a `PeristentVolume`
from which a `PersistentVolumeClaim` can be made.

If that whole paragraph feels like I'm talking in circles- that's OK for now. Here is the important thing you need to
understand:

IMPORTANT: The `PeristentVolumeClaim` is where the trained model is stored, and then loaded for serving.


=== Training and Deploying a Model

In traditional "machine learning" texts, this phase is the one that most of the book is dedicated to, with a few simple
examples on deployment, and very little treatment on model management.  In this text however, we assume chosing what
algorithm/model to use is either something the practitioner is very familiar with or not the practitioners job, and so
they don't really care.  We will point out where the reader may see the training code as we drive by and not dive into it at all.

==== Building a Training Image

Let's head back to the
`example-seldon/models/sk_mnist/train` directory.  In this directory you'll see a Dockerfile, and `create_model.py`.
`create_model.py` downloads the MNIST dataset, train a RandomForrestClassifier on the data within, and saves
the trained model to `/data/sk.pkl`, which is our `PersistentVolumeClaim`.   The Dockerfile installs `requirements.txt`
and waits for `/data` to be mounted before running `create_model.py`.

While this process is intersting, and worth taking a look at, it is not required for this simple example.  We only point
it out to show that there is no magic happening here, and to give the reader a preview of things to come.

==== Training and Monitoring Training Progress

The next step is to train the model.  This is done via an `argo` workflow.  This will take the training container we just
discussed, and launch it in the cluster. There are various reasons for wanting to conduct training on a cluster instead
of a laptop including, the data exists in a place adjacent to the Kubernetes cluster or you need more power than your laptop
can provide, to name a few.

In the <<create_training_workflow>>, we see how to submit the model training workflow with `argo`.

.Create Training Workflow Example
[[create_training_workflow]]
====
[source, shell]
----
include::examples/ch2_seldon_examples/tagable_bs.sh[tags=createTrainingWorkflow]
----
====

There are two ways to "monitor" the progress of training.  The simplest is to do so via the CLI. If the later GUI methods
don't work for you, please don't fret- simply 'watch' the progress via the command line interface as is shown in
<<monitor_training_workflow_cli>>.

.Monitor Training Workflow Example
[[monitor_training_workflow_cli]]
====
[source, shell]
----
include::examples/ch2_seldon_examples/tagable_bs.sh[tags=cliTrainingCheck]
----
====

The CLI (Command Line Interface) is a perfectly acceptable way to monitor a job, but there is a prettier option available.

IMPORTANT: There are as many ways for this next part to go wrong as their are places to deploy Kubernetes (i.e. as there
are stars in the sky).  The important thing is that your model trains.  Try this next part but if it doesn't work, that
is OK. Don't get hung up here if it doesn't work out perfect.

You can go to the `ambassador` UI, and add `/argo/` to the end of the URL. If you are running Kubernetes in `minikube` or
`microk8s`, you will need your IP address (or your VMs IP address), and the `ambassador` port.  To get your `ambassador`
port follow <<get_ambassador_port>>.

.Getting Ambassador Port Example
[[get_ambassador_port]]
====
[source, shell]
----
include::examples/ch2_seldon_examples/tagable_bs.sh[tags=getAmbassadorPort]
----
====

The URI you want is http://<machine_ip>:<ambassador_port>/argo/

For me that is http://10.53.148.167:30134/argo/  The observent reader will notice, that the `ambassador` IP address listed
in the prior example, is not the same as the machine IP address.

WARNING: The machine IP address is not the same as the ambassador IP address.

If everything goes correctly (and it's OK for now if it doesn't), you will see something like this.

.Argo WebUI
[#img-argo]
[caption="Figure 1: "]
image::images/ch2_argo_screenshot.png[argo-ui,909,510]



==== Building and Deploying Serving Images (I'mt here)

note why, how, expected results

```bash
argo submit serving-sk-mnist-workflow.yaml -n kubeflow -p deploy-model=true
```

TIP: `-p deploy-model=true` is a feature of this particular workflow, but not nessicary for all kubeflow serving.


```bash
kubectl get pods -n kubeflow | grep sk-deploy
```

==== Test Query

note why, how, expected results

```bash
$ kubectl get services -n kubeflow | grep ambassador
ambassador                                           NodePort    10.152.183.112   <none>        80:30134/TCP        61m
ambassador-admin                                     ClusterIP   10.152.183.204   <none>        8877/TCP            61m
```

The model will be available at http://<MACHINE_IP>:<AMBASSADOR_PORT>/seldon/<DEPLOYMENT_NAME>/api/v0.1/predictions

There is a python script available at (insert path, it's in atlas) for pulling an image from the MNIST dataset, turning
into a vector, displaying the image, and sending to to the model.  The model returns a json of the 10 digits and the
probabiltiy that submitted image is what digit.

http://<MACHINE_IP>:<GRAFANA_PORT>/dashboard/db/prediction-analytics?refresh=5s&orgId=1
uid admin, pwd password


Load test:
```bash
kubectl label nodes $(kubectl get nodes -o jsonpath='{.items[0].metadata.name}') role=locust

helm install seldon-core-loadtesting --name loadtest  \
    --namespace kubeflow \
    --repo https://storage.googleapis.com/seldon-charts \
    --set locust.script=mnist_rest_locust.py \
    --set locust.host=http://mnist-classifier:8000 \
    --set oauth.enabled=false \
    --set oauth.key=oauth-key \
    --set oauth.secret=oauth-secret \
    --set locust.hatchRate=1 \
    --set locust.clients=1 \
    --set loadtest.sendFeedback=1 \
    --set locust.minWait=0 \
    --set locust.maxWait=0 \
    --set replicaCount=1 \
    --set data.size=784
```


```
{'data': {'names': ['class:0',
                    'class:1',
                    'class:2',
                    'class:3',
                    'class:4',
                    'class:5',
                    'class:6',
                    'class:7',
                    'class:8',
                    'class:9'],
          'ndarray':[[0.03333333333333333,
                      0.26666666666666666,
                      0.03333333333333333,
                      0.13333333333333333, ## It was actually this
                      0.1,
                      0.06666666666666667,
                      0.1,
                      0.26666666666666666,
                      0.0,
                      0.0]]},
 'meta': {'puid': 'tb02ff58vcinl82jmkkoe80u4r', 'routing': {}, 'tags': {}}}
```

==== Things to come

Preview of chapter X

=== Conclusion

We hope that your first adventure into Kubeflow has been a success.
In the next chapter we are going to introduce the case studies that we will develop
throughout the rest of the book.
