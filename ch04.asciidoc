[[data_and_feature_prep]]
==  Data and Feature Preparation

Machine Learning, especially deep learning, depends on large amounts of data to create models.
While different types of machine learning techniques can handle different kinds of features (e.g. a linear regression model isn't going to be able to work directly with binary image data), data and feature preparation will be an important part of your machine learning.
If you ask machine learning partictioners, many of them spend the majourity of their time working on a combination of data and feature preparation.


Data preparation refers to the act of sourcing the data, augmenting with other datasets, and handling missing/bad/partial records.
This part of the machine learning process can consume a large amount of time, and is often much less fun than the other steps.
That being said, just because this part is not enjoyable does not mean you should give it short shift, high quality data can be the difference
between a successful ML project and a dissapointment.


Feature preparation (sometimes also called feature engineering) refers to the act of transforming the raw input data into features which your machine learning model can use.
Feature preparation often depends on knowledge of both the data and the kind of machine learning algorithms you plan to use.
For example, when training a linear regression model, a common part of feature engineering may be encoding explicit interactions we suspect may be useful, however it is unlikely doing this would benefit a deep-learning model to the same degree.


Both of these processes are best approached iteratively, revisiting them as your understanding of the problem and model changes.
While we examine data preparation and feature preparation seperately, it is not uncommon for these to exist as a single integrated component,
especially in situations where the datasets are less complex.




=== Data Preparation

Data preparation can generally be viewed as two distinct stages: collecting the data, and cleaning/filtering the data.
Often this can be an iterative process later on in the process we may notice the unexpected behaviour in the model, which can be traced back to bad data in our input.
Putting in the effort here to collect both more records and more information about each record here can make huge improvements in your model footnote:[See: The Unreasonable Effectiveness of Data by Halevy, norvid and Pereira, More Data beats Better Algorithms by Tyler Schnoebelen, and many more].

[TIP]
====
Once you've got your data ready, it's important to make sure that future iterations do not have new errors in data introduced. See <<validating_training_data>> for a discussion.
====


==== Deciding on the correct tooling

With two very distinct paths of tooling, making the wrong decision hear can involve substantail changes.
If your input data size is relatively small, a single machine offers you all of the tools you are familiar with.
Larger data sizes tend to require distributed systems, or very large containers.
Even with smaller data sets, distributed systems, like Apache Spark, Flink, or Beam, can do data preparation faster, but can require learning new tools.

You need not use the same tool for all of your data preparation.
This is especially common when working with multiple data sets of different types on which the same tools would be inconvient to use.
A common approach in those situations is to do a first pass data dump into a more accessiable format which your primary tool of choice can work with.
Thankfully Kubeflow Pipelines, introduced in <<pipelines_ch>> , gives you the tools to connect the different steps in an automated manner.



==== Where to put your data between steps

Data preparation in and off its self is not enough for a machine learning solution, so we need to be able to put the output of our data preparation somewhere that it can be consumed later.
In many of the Kubeflow examples, persistant volumes are used to store the data between the data preparation and the next stage.
This is beneficial since it does not depend on a specific storage vendor, however in many situations persistant volumes can be:
// TODO(holden)
slow (TODO link), difficult to scale (TODO link), may require additional set up (link to the mnist seldon example), and on many providers may not support multiple concurrent writers.
While less common in the Kubeflow examples, using the object storage solution of your cloud (e.g. S3/Azure Blob Storage/GCS) may be more suitable.

[WARNING]
====
Using non-kubernetes integrated storage layer does mean that moving your pipeline between clouds can require some changes.
====


==== Single Machine Data Preparation

// Which dataset is the smallest? Let's do the example with that on a single machine.
// Or put in the GH data but from one day

Doing data preparation on a single machine limits the scale of data we are able to handle, but offers the widest range of tools.
If you're an experienced Python programmer a common way to do data preparation is with Jupyter notebooks.

===== Using Jupyter

To get started you can use the Jupyter Hub installation on Kubeflow to begin the development of your data ingestion step, and once it's finished
you can package it into a repeatable container.


The easist way to connect to Jupyter is through the Ambassador Web UI. If you've deployed on GCP a proxy should have been created with the name of your deployment `${DEPLOYMENTNAME}.endpoints.${PROJECTNAME}.cloud.goog`.
Otherwise you can use kubernetes port forwarding to access the Ambassador:

.Ambassador port forward
====
[source, bash]
----
include::examples/multi-cloud/solution1.sh[tags=connectambassador]
----
====

Then connect to localhost on 8080.

.Ambassador Web UI
[[#ambassador_web_ui]]
image::images/4_kubeflow_ambassador.png

From the ambassador web ui <<#ambassador_web_ui>> you can launch a new Jupyter hub server.
When you launch a Jupyter Hub instance you'll be able to select from a list of containers to match the requirements you need.

.Jupyter Launcher Page Image
[[#jupyter_launcher_page_image]]
image::images/4_newnotebook_server.png

The same containers can later be used as the basis productionize your pipeline you can use a compatible base contain.


===== Building a Container

Regardless of if you've started with a notebook, or have another data extraction program you can productionize it in a container.
Containers are composable, which means we can build our container on top of each other.


The containers that are used for Jupyter hub can be used to run your notebook programatically.
The Dockerfile will look something like:

.Dockerfile for Tensorflow Notebook
====
[source, dockerfile]
----
include::/examples/data-extraction/iot/Dockerfile[tags=spec]
----
====


.Building & uploading your container image

.Writing a job

While this job is fairly simple, we can also use `kustomize` to generate jobs based on templates.

.Kustomize base template

.Kustomize job



This works well for a notebook, but when productionizing other workloads it can be a bit more complicated.
For the GitHub dataset, we're sourcing our data from Google's BigQuery with a few simple queries, including <<github_push_events_bql>>.

[[github_push_events_bql]]
.Query Push Events
====
[source, sql]
----
include::/examples/data-extraction/github/extract_bq/github_push_events.bsql
----
====


As with the notebook example, we can find a container to build on top of rather than starting from scratch.
Since the only external tool we need is Google's big query, we can use `google/cloud-sdk:247.0.0` as the basis, which includes most of what we need.
On top that we want to put our queries and a script to execute them into the container which we can specify in a Dockerfile <<github_run_queries_dockerfile>>.

// TODO(holden): Should we include the shell script?

[[github_run_queries_dockerfile]]
====
[source, dockerfile]
----
include::/examples/data-extraction/github/extract_bq/Dockerfile[tags=base]
----
====

From this dockerfile we can build it with <<build_gh_container>>:

[[build_gh_container]]
====
[source, dockerfile]
----
include::/examples/data-extraction/github/extract_bq/build_manual.sh[tags=build]
----
====

Now if we want to verify our image outside of Kubeflow we can manually run the container <<run_gh_locally>>.

[[run_gh_locally]]
====
[source, dockerfile]
----
include::/examples/data-extraction/github/extract_bq/build_manual.sh[tags=manualrun]
----
====


Once we have our container created we then need to publish it to a container registry so Kubeflow can access it <<push_to_registry>>.

[[push_to_registry]]
====
[source, dockerfile]
----
include::/examples/data-extraction/github/extract_bq/build_manual.sh[tags=push]
----
====


To make this run-able in Kubernetes we need to write a controller, as with the Jupyter example we can use the `Job` type which is designed for running until completion.

[[add_credentials]]
====
[source, dockerfile]
----
include::/examples/data-extraction/github/extract_bq/simple_job_no_perm.yaml[]
----
====


The default Google container service account in Kubeflow doesn't have enough permissions to read from BigQuery and write to Google cloud service.
Kubeflow creates a default `user-gcp-sa` account & secret which thankfully has the permissions we need.
To configure our container & script to use this we need to mount a Kubernetes secret and set the `GOOGLE_APPLICATION_CREDENTIALS` enviroment variable to point to the correct secret.


[[add_credentials]]
====
[source, dockerfile]
----
include::/examples/data-extraction/github/extract_bq/simple_job.yaml[]
----
====


Our data extraction shell script reads the `PROJECT` & `BUCKET` environment variables which we may want to override from the default.
To support this we add an `env` entries into our `job.yaml`:

[[add_new_envs_for_project_and_bucket]]
====
[source, dockerfile]
----
include::/examples/data-extraction/github/extract_bq/simple_job_configured.yaml[tags=newenvs]
----
====


Once we've made a manual job we can go ahead and make it configurable with kustomize.
The first step is making a copy of `job.yaml` into a new directory giving it a name like `deployment.yaml` and making the elements we want to have replaced with `$(paramName)`.
For the GitHub pipeline this looks like <<gh_template_bucket_example>>.

[[gh_template_bucket_example]]
====
[source, dockerfile]
----
include::/examples/data-extraction/github/extract_bq/bucket_kustomize/deployment.yaml[]
----
====

This gives us a template that we can use Kustomize on, but we need to tell configure Kustomize.

[[gh_kustomize]]
====
[source, dockerfile]
----
include::/examples/data-extraction/github/extract_bq/bucket_kustomize/kustomization.yaml[]
----
====



To tell Kustomize to substitute in specific values for the `$(paramName)` we add a `params.yaml` that specifies the paths of the different entries we want it to template.

.Specifying Parameters
[[specify_params]]
====
[source, yaml]
----
include::/examples/data-extraction/github/extract_bq/bucket_kustomize/params.yaml[]
----
====

Now that the kustomize file is created, we can now build a specific job based on command line parameters and run it.

.Configure and run the job
[[run_gh_job]]
====
[source, sh]
----
include::/examples/data-extraction/github/extract_bq/build_manual.sh[tags=run]
----
====

And we can verify the job result:

.Look for the job
[[verify_gh_job]]
====
[source, sh]
----
include::/examples/data-extraction/github/extract_bq/build_manual.sh[tags=verify]
----
====


[NOTE]
====
Google BigQuery is only available on Google cloud. The same result be accomplished on other providers by using Apache Spark directly on the raw JSON dumps.
====

Sometimes you won't be able to find a container that so closely matches your needs as we did.
In those cases you can take a generic base image, I prefer Ubuntu because I'm familiar with it others like Alpine, and build your requirements on top of it.
In those situations, in addition to copying values in like in <<github_run_queries_dockerfile>>, you can also run commands to set up your container with `RUN`.
Instead of using `google/cloud-sdk:247.0.0` we could build a `Dockerfile` to install `google-cloud-sdk` on top of Ubuntu <<alt_dockerfile>>.

[[alt_dockerfile]]
.Dockerfile to install google-cloud-sdk on Ubuntu
====
[source, docker]
----
include::data-extraction/github/extract_bq/AltDockerfile[]
----
====



[WARNING]
====
New version of Docker on Ubuntu default to non-privileged mode which restricts our ability to install packages. If you get permission denied errors re-run the command with `--privileged`.
====




==== Distributed Data Preparation w/Apache Spark


Using a distributed platform make it faster to ingest and process very large data sources.
In Kubeflow, at present, the three data parallel distributed systems available for data preparation are: Apache Spark, Google's Dataflow (via Apache Beam), and link:$http://docs.pachyderm.io/en/latest/fundamentals/distributed_computing.html$[Pachyderm].
For now we will focus on Apache Spark, but many of these data parallel systems have similar concepts if different syntax & system requirements.

[TIP]
====
We briefly introduce Spark's new Dataset/Dataframe APIs and the tools to load & save your data.
If you want to go beyond the basics we recommend Learning Spark, Spark the Definitive Guide, or High Performance Spark as resources to improve your Spark skills.
====


There are two different ways to get use Spark inside of Kubeflow,
one is with the spark operator component inside of Kubeflow,
and the other is by using a Jupyter notebook with Spark.
In my opinion, a Jupyter notebook is a great place to start to understand your data,
however Jupyter notebooks make important activities like testing and version management more
challenging and can quickly become difficult to maintain.

// TODO: holden -- add an example of using a Jupyter notebook with Spark in Kubeflow

The other way to do this is build a classic program to do your data preparation.
If you've started with a notebook but found it difficult to maintain you can move your notebook into a traditional program.

===== Reading the Input Data

Apache Spark has APIs available in Python, R, Scala, Java; with some 3rd party support for other languages.
We'll use the Python interface due to it's popularity in the machine learning community.
Spark supports a wide variety of data sources, including (but not limited to):
Parquet, JDBC, ORC, JSON, Hive, CSV, ElasticSearch, MongoDB, Neo4j, Cassandra, Snowflake, Redis, Riak Time Series, etc.


Some of the most common data sources are built into Apache Spark and can be loaded directly as with:

// TODO EXAMPLE

However, some data sources (such as X*) require additional settings for Spark to download the data.


Four our GitHub example, the data is stored in big query or as JSON dumps.
Spark directly supports reading JSON, but we can do some pre-filtering in BigQuery SQL on the data to simplify our pipeline.
To access the resulting BigQuery data


===== Validating the schema


===== Handling missing fields

In many situations some of our data will be missing or need computing.
Depending on our dataset we can either fill in those missing data fields with lower quality fields, or compute them, or
if the field is core we can reject records which don't have the required field present.


The GitHub example does a combination of all 3 of these.


===== Filtering out bad data

Detecting incorrect data can be challenging, however without it

===== Joining and augmenting the data


===== Saving the output

Once you have your data ready it's often time to save the output. If you're going to use Apache Spark to do your feature preparation you can build a program to do both.
If, after filtering, the amount of data is relatively small and you wish to use a persistant volume to save your data you can bring the data back to the driver program by calling `collect()` and saving it as a regular collection.
If your data is large, or you otherwise want to use an object store, Spark can write out to many different format just as in the loading.
The correct format will depend on the tool you intend to use for feature preparation.



=== Feature Preparation

==== Using TFX / Tensorflow Transform


The Tensorflow community has created an excellent set of integrated tools for doing data preparation.
At present these tools are all built on top of Apache Beam and have limited support for distributed processing outside of Google Cloud.
However if you have relatively small datasets these tools can be used regardless of where your Kubernetes cluster is deployed.


[TIP]
====
Apache Beam's support for Apache Flink is under active development and _could_ mean that future versions are able to work outside of Google Cloud.
====


==== Using Apache Spark

Apache Spark has a large number of built in feature preparation stages that you can use


[TIP]
====
While we've looked at using Apache Spark for feature prep you can also use it for training and serving a variety of machine learning models we explore in <<spark_ch>>
====

=== Conclusion

Now you have the skills to get your data ready for doing machine learning on.
As you approach the next chapters you will have a choice of choosing which kind of tools you want to build your model with.

