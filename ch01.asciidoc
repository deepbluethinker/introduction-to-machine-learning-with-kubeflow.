[[who_is_kubeflow_for_ch]]
== What does Kubeflow solve & who’s it for, and how to get started


Put some introductory text here.

=== Who is this for?

Kubeflow is for data scientist and data engineers who wish to avoid the difficulty updating their model with new data, or moving from their laptop to production.
Kubeflow does this by offering a variety of tools in one place, more than just Tensorflow, for all the different stages of a models life (data exploration, feature prep, model training, model serving, and model testing).

[TIP]
====
We talk about who this book is for in <<assumptions_about the audience>> as well as the required background and resources to get up to speed if you need them.
====

[[breif_kubeflow_design_and_core_components]]
=== Kubeflow’s Design & Core Components

In the machine learning landscape, there are many diverse tool sets and frameworks. Kubeflow does not seek to reinvent the wheel or provide a "one size fits all" solution - instead, it allows machine learning practitioners to compose and customize their own stacks based on specific needs. It is designed to simplify the process of building and deploying machine learning at scale; allowing data scientists to do their jobs without worrying about the "boring parts" of building the infrastructure.

Kubeflow seeks to tackle the problem of simplifying machine learning through three aspects: composability, portability, and scalability.

* Composability - the core components come from data science tools that are already familiar to machine learning practictioners. They can be used independently to facilitate specific stages of machine learning, or composed together to form end-to-end pipelines.

* Portability - taking advantage of Kubernetes and cloud-native microservices, Kubeflow does not require you to anchor to any particular platform. You can develop and prototype on your laptop, and deploy to production effortlessly.

* Scalability - the container-based design allows your clusters to dynamically scale up or down according to demand.


Let's take a closer look at some of these components.

==== Training

Kubeflow supports a variety of distributed training frameworks. As of the time of this writing, Kubeflow has support for:

* TensorFlow
* PyTorch
* MXNet
* Chainer
* Caffe2
* MPI


In Kubeflow, distributed training jobs are managed by application-specific controllers, known as "operators". These operators extend the Kubernetes APIs to create, manage, and manipulate the state of resources. For example, to run a distributed TensorFlow training job, the user just needs to provide a specification that describes the desired state (number of workers and parameter servers, etc), and the TensorFlow operator component will take care of the rest and manage the lifecycle of the training job.

In a later chapter we will examine how Kubeflow trains a TensorFlow model in greater detail.

==== Serving

After training your model, the next step is to serve the model in your cluster so it can handle prediction requests. Kubeflow makes it easy for data scientists to deploy machine learning models in production environments at scale. Currently Kubeflow supports TensorFlow Serving and SeldonCore.

Serving a model on Kubeflow is fairly straightforward. There is no need to build or customize a container yourself - simply point Kubeflow to where your model is stored, and a server will be ready within seconds to service requests.

==== Hyperparameter Tuning

In machine learning, hyperparameters are variables that govern the training process. For example, what should the model's learning rate be? How many hidden layers and neurons should be in the neural network? These parameters are not part of the training data, but they can have a significant effect on the performance of the training models.

Finding the right set of hyperparameters for your training model can be a challenging task. Traditional methodologies such as grid search can be time consuming and quite tedious.

Kubeflow provides a component (called "Katib") that allows users to perform hyperparameter optimizations easily on Kubernetes clusters. Katib is inspired by Vizier, a black-box optimization framework created at Google. It leverages advanced searching algorithms such as Bayesian optimization to find optimal hyperparameter configurations.

With Kubeflow, users can begin with a training model that they are unsure about, define the hyperparameter search space, and Kubeflow will take care of the rest - spin up training jobs using different hyperparameters, collect the metrics, and save the results to a model database so their performance can be compared.


==== Jupyter Notebooks

Jupyter notebook is an open-source web application that allows users to create and share data, code snippets, and experiments at scale. They are popular among machine learning practitioners due to their simplicity and portability.

.A Jupyter notebook running in Kubeflow
[#1_jupyter_notebook]
image::images/1_jupyter_notebook.png[A Jupyter notebook running in Kubeflow]

In Kubeflow, you can spin up instances of Jupyter notebooks that directly interact with your cluster and its other components. For example, you can write snippets of TensorFlow distributed training code in your laptop, and bring up a training cluster with just a few clicks.

==== Pipelines

A machine learning pipeline can be seen as a graph, where each node is a stage in a workflow. Kubeflow Pipelines is a component that allows users to compose reusable workflows at ease. Its features include:

* An orchestratration engine for multi-step workflows

* An SDK to interact with pipeline components

* A user interface that allows users to visualize and track experiments, and to share results with collaborators


.A Kubeflow pipeline
[#1_kubeflow_pipeline]
image::images/1_kubeflow_pipeline.png[A Kubeflow pipeline]

Each building block in a pipeline is a self-contained step in a machine learning workflow, such as preprocessing, transformation, training, or cross-validation. In Kubeflow, each of these blocks is built as a Docker image. This allows users to share and reuse individual components with ease.

=== Getting Set Up with Kubeflow

One of the great things about Kubeflow is the ability to do our initial development and exploration locally, moving into more powerful and distributed tools later on.
Deploying Kubeflow locally can be a great way to get started, and gives us access to a few debugging cheats we can use during our initial exploration.
While you can get started with Kubeflow locally, you don't have to, you can just as easily do your initial work with one of the cloud providers or on prem Kubernetes clusters covered in <<appendix_installing>>, although for all but click-to-deploy.

[TIP]
====
Probably the fastest way to get started with Kubeflow is using the simplified click-to-deploy on Google Cloud Platform (GCP). If you're in a rush to get started go ahead and jump to <<gcp_click_to_deploy>>.
====


==== Installing Kubeflow & its dependencies

Kubeflow has relatively few dependencies, most of the heavy lifting is done inside of containers which ship with their requirements.
Thankfully, Kubeflow's dependencies can be automatically installed along with the optional Minikube by running the install shell script <<mini_kube_setup_sh>>. Even if you don't plan on using Minikube this script simplifies the rest of the set up.

[[mini_kube_setup_sh]]
.Automatically install Kubeflow's dependencies & Minikube
[source, shell]
----
include::examples/dev-setup/install-kf-and-minikube.sh[tags=work]
----

[TIP]
====
If this fails to X/Y/Z, don't worry we'll cover how to fix that up in <<setting_up_minikube>>.
====



[WARNING]
====
This will modify your system packages installing and require root/super user permissions. If you wish you can instead run these commands on another host (e.g. dedicated KF development enviroment).
====

If you don't want to install Minikube, you can manually install Kubeflow's dependencies: ksonnet and kubectl. Even if you have these installed, you will want to check that you have the right versions installed for your version of Kubeflow.
// TODO(holden): Where? Release notes?
Ksonnet is on github, with its release posted at link:$$https://github.com/ksonnet/ksonnet/releases$$[https://github.com/ksonnet/ksonnet/releases], and you can install it with <<install_ksonnet_sh>>.

[[install_ksonnet_sh]]
.Install ksonnet
[source, shell]
----
include::examples/dev-setup/install-ksonnet-and-kubectl.sh[tags=ksonnet]
----



Kubectl is more widely packaged, with the different installation options covered in the link:$$https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl$$[kubernetes documentation https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl]. Ubuntu (snap) <<install_kubectl_snap>> and Homebrew users can install Kubectl <install_kubectl_homebrew>> the simplest with package manager, with others covered in the guide.
Kubectl can also be installed outside of your package manager <<install_kubectl_curl>>, although this requires manual updating.

[[install_kubectl_snap]]
.Install kubectl with snap
[source, shell]
----
include::examples/dev-setup/install-ksonnet-and-kubectl.sh[tags=ubuntu-kubectl]
----

[[install_kubectl_homebrew]]
.Install kubectl with homebrew
[source, shell]
----
include::examples/dev-setup/install-ksonnet-and-kubectl.sh[tags=osx-kubectl]
----

[[install_kubectl_curl]]
.Install kubectl without a package manager
[source, shell]
----
include::examples/dev-setup/install-ksonnet-and-kubectl.sh[tags=no-pkg-manager-kubectl]
----




Once you have the dependencies installed, you can now install Kubeflow with <<install_kf_sh>>, and optionally add it to your path.

[[install_kf]]
.Install Kubeflow
[source, shell]
----
include::examples/dev-setup/install-kf.sh[tags=no-pkg-manager-install]
----

You now have Kubeflow installed! There are some additional optional tools you can set up to make the rest of this book easier we will cover next.

[[setting_up_minikube]]
==== Finishing setting up Kubeflow on Minikube (optional)

The Kubeflow project has a link:$$https://www.kubeflow.org/docs/started/getting-started-minikube/$$[frequently updated guide on how to install Kubeflow with Minikube at https://www.kubeflow.org/docs/started/getting-started-minikube/].
You don't need to have a local Kubernetes cluster, but many data scientists and developers find having a local cluster to test with (be it Minikube or other).
// TODO: other local k8s options



==== Setting up your Kubeflow development enviroment

In addition to the core dependencies you may find some additional tools useful.
Much of Kubernets and Kubeflow is configured with
either link:$$https://yaml.org/$$[YAML Ain't Markup Language (YAML)]
or link:$$https://ksonnet.io/$$[ksonnet] (which is an extension of link:$$https://jsonnet.org/$$[Jsonnet]).

[TIP]
====
This part is optional, if it ends up being frustrating or slowing you down feel free to skip ahead.
====


Inside of the Kubeflow project, the Jsonnet library is used to automatically format Ksonnet into a standard layout. This autoformatter can also catch syntax errors or other simple errors earlier on in the deploy. The jsonnet package link:$$https://github.com/google/jsonnet/releases$$[releases are available on GitHub] <<jsonnet_manual>>, link:$$https://snapcraft.io/$$[Snap] <<jsonnet_snap>> (from Canonical). It is also listed on PyPi, but currently does not install the command line tools. Some of the IDE tools for Jsonnet depend on the command line binary.

[[jsonnet_snap]]
.Install jsonnet with snap (Ubuntu)
====
[source, shell]
----
include::examples/dev-setup/jsonnet.sh[tags=snap]
----
====

[[jsonnet_manual]]
.Install jsonnet manually (everyone else)
====
[source, shell]
----
include::examples/dev-setup/jsonnet.sh[tags=manual]
----
====


Most Integrated Development Enviroments (IDEs) offer some sort of tooling for editing YAML and Jsonnet, but you may have to install these seperately.
For InteliJ //TODO(trevor)
For emacs there are many modes available for YAML editing, as well as the link:$$jsonnet-mode$$[jsonnet] (which is installable from link:$https://melpa.org/$$[Milkypostman’s Emacs Lisp Package Archive (MELPA)]).
Atom has syntax highlighting available as packages for both link:$$https://atom.io/packages/language-yaml$$[YAML] and link:$$https://atom.io/packages/language-jsonnet$$[Jsonnet].
If you use a different IDE don't throw it away just for better ksonnet editing before you explore the plugin available.

[[brief_alternatives_to_kubeflow]]
=== Brief Alternatives to Kubeflow

Within the research community, various alternatives exist that provide uniquely different functionality to that of Kubeflow.
In recent research cadences, model development or training has received much attention and support with regards to advancements in infrastructure, theory, and systems.
Prediction serving, on the other hand, has received relatively less attention.
As such, it is commonly seen that data science practitioners end up hacking together an amalgam of critical systems components that are integrated to support serving and inference across various workloads and continuously evolving frameworks.
However, given the demand for constant availability and horizontal scalability, solutions like Kubeflow and various others are gaining traction through industry as powerful architectural abstraction tools and as convincing research scopes.

==== Clipper (RiseLabs)
One interesting alternative to Kubeflow, is a general-purpose low-latency prediction serving system that came out of RiseLabs, titled Clipper. In an attempt to simplify deployment, optimization, and inference, Clipper has developed a layered architecture system like the one seen below.
image::images/6_clipper.png[Clipper and its modular architecture]
Through various optimizations and its modular design, Clipper, in its seminal paper, has achieved low latency and high throughput predictions at comparable levels to TensorFlow Serving, on three TensorFlow models of varying inference costs.

Clipper is divided across two abstractions, aptly named: model selection and model abstraction layers.

The model selection layer is quite sophisticated in that it uses an adaptive online model selection policy and various ensemble techniques to incorporate feedback and automatically select and combine predictions
from models that can span multiple machine learning frameworks. Since you are continuously learning from feedback throughout the lifetime of your application, the model selection layer self-calibrates failed model without needing to interact directly with the policy layer.
The two different selection policies that are implemented allow for single-model selection, using a multi-armed bandit algorithm, and ensemble-model selection, using bootstrap aggregation (or bagging).

The model abstraction layer provides an implementable interface for providing pluggability in terms of custom machine learning frameworks.
The layer is comprised of a prediction cache, an adaptive query-batching component, and model containers that are communicable through RPC.
These model containers are built as Docker containers, so it was intended that Clipper may run on Kubernetes seamlessly.

Clipper's modular architecture and focus on containerization, similar to Kubeflow, enables caching and batching mechanisms to be shared across frameworks while also reaping the gains of scalability, concurrency, and flexibility in adding new model frameworks

Graduating from just scientific theory into a functional end-to-end system, Clipper has gained traction within the scientific community and has had various parts of its architectural designs incorporated into recently introduced machine learning systems; nonetheless, we have yet to see if it will be adopted in industry at scale.

==== MLflow (Databricks)

MLflow is Databrick's own flavor of open-source machine learning development.
The architecture of MLflow leverages a lot of the same architectural paradigms of Clipper, including its framework agnostic nature,  while focusing on three major components that it calls: Tracking, Projects, and Models.

MLflow Tracking functions as an API with a complementing UI for logging parameters, code versions, metrics and output files. This is quite power in machine learning as tracking parameters, metrics, and artifacts is of paramount importance.
The power of this API is that it can be leveraged in a standalone or notebook setting.

MLflow Projects provides a standard format for packaging reusable data science code, defined by a yaml file that can leverage source-controlled code and dependency management via Anaconda.
The project format makes it easy to share reproducible data science code, as reproduciblity is critical for machine learning practitioners.

MLflow Models is a convention for packaging machine learning models in multiple format.
Each MLflow Model is saved as a directory containing arbitrary files and an MLmodel descriptor file.

MLflow is still in active development, but with the entire project being open-sourced, the visibility and community excitement around its development is enticing as well.

==== Others

Because of the challenges presented in machine learning development, many organizations have started to build internal machine learning platforms to manage their machine learning lifecycle.
For example: Bloomberg, Facebook, Google, Uber have built the Data Science Platform, FBLearner Flow, TFX, and Michelangelo to manage data preparation, model training and deployment.
With the machine learning infrastructure landscape always evolving and maturing, we are excited to see how open-source projects, like Kubeflow, will bring much-needed simplicity and abstraction to machine learning development.

[TIP]
====
If you get stuck here, or if your permissions/policies don't allow you to use minikube, just jump on over to <<appendix_installing>> and you can deploy your Kubeflow app's against your remote K8s cluster.
====


=== Getting Help / Community Resources

We are so glad you've decided to use this book to start your adventures into Kubeflow.
However, like all adventures, there may come a point when your guide book isn't enough to carry you through.
Thankfully there are a collection of community resources where you can interact with others on similar adventures to your own.
We encourage you to sign up for the link:$$http://kubeflow.slack.com$$[Kubeflow slack at http://kubeflow.slack.com], one of the more active areas of discussion, now since account approval is semi-manual.
The link:$$https://www.kubeflow.org/$$[kubeflow project page is at https://www.kubeflow.org/].

[TIP]
====
If you want to quickly explore Kubeflow end-to-end there are some link:$$https://codelabs.developers.google.com/$$[Google codelabs] with Kubeflow as well.
====

=== Conclusion

At this point you have everything you need to get started on your Kubeflow adventure: a Kubernetes cluster, Kubeflow and its core dependencies, and a desire to have fun.
In the next chapter <<simple_training_ch>>, we'll train and serve a relatively simple machine learning model together to give you an idea of how the basics of how to use Kubeflow.
