[[who_is_kubeflow_for_ch]]
== Kubeflow: What it is, who is it for & getting started

Kubeflow solves the problem of how to take machine learning labs from research to production.
Kubeflow does this by giving you the tools to build portable, reliable, scalable machine learning pipelines and using the results.
Despite common misconception, Kubeflow is more than just Kubernetes and TensorFlow -- you can use it for all sorts of machine learning models.
We hope Kubeflow is the right tool for you, although it may not be especially if your organization does not have Kubernetes deployed.
The <<brief_alternatives_to_kubeflow>> introduces some options you may wish to explore.


This chapter aims to help you decide if Kubeflow is the right tool for your problem.
We'll cover the benefits you can expect from Kubeflow, some of the alternatives, and some of the costs of using kubeflow.
Once you've decided on Kubeflow, or at least exploring it, we'll show you how to get started quickly on your local computer,
and direct you to the correct appendix for the cloud or Kubernetes provider of your choice for scaling.



=== What is Kubeflow for & who should use it?

Kubeflow is for both data scientists and data engineers looking to build production-grade machine learning pipelines.
Kubeflow packages components for all the different stages of a model's lifecycle (data exploration, feature preparation, model training, model serving, and model testing); allowing users to build integrated end-to-end pipelines.


Kubeflow can be run either locally in your development environment or on a production cluster once the pipelines are ready.
Kubeflow provides a unified system leveraging Kubernetes for scalability and containerization for portability and repeatability of pipelines.


=== Why Containerization?

Containerization means no more "it worked on my machine" or "oh yeah, we forgot about just one, you need this extra package."
Containerized applications are isolated from the rest of your machine, and have all of their requirements included (from the operating system up).


A common worry about using containers is overhead, but containers can be built in layers allowing you to use another container description as its base.
This means that if, for example, you have a new NLP library you want to use in your pipeline you can add it on top of the existing container and you don't have to start from scratch each time.


[TIP]
====
Data scientists with Python experience can think of containers as a heavy duty virtual environment, which includes the operating system, the packages, and everything in between.
====

The isolation provided by containers allow our machine learning stages to be portable and reproducible.

=== Why Kubernetes?

Kubernetes allows our pipelines to be scalable without sacrificing portability, allowing us to avoid becoming locked into a specific cloud provider.
In addition to being able to switch from a single machine to a distributed cluster, different stages of your machine learning pipeline can request different amounts and types of resources - for example, your data preparation step may benefit more from running on multiple machines, while your model training can benefit more from access GPUs.
This flexibility is especially useful in cloud enviroments, where you can reduce your costs by only using expensive resources when required.


You can, of course, build your own containerized machine learning pipelines on Kubernetes without using Kubeflow, but Kubeflow aims to make this substantailly easier.
Kubeflow provides a partially-standardized interface to the tools you would be likely to use on your own adventure, as well making it easier to configure your pipeline to use cloud accelerations like TPUs (tensor processing units).



[[breif_kubeflow_design_and_core_components]]
=== Kubeflow’s Design & Core Components

In the machine learning landscape, there are many diverse tool sets and frameworks. Kubeflow does not seek to reinvent the wheel or provide a "one size fits all" solution - instead, it allows machine learning practitioners to compose and customize their own stacks based on specific needs. It is designed to simplify the process of building and deploying machine learning at scale; allowing data scientists to do their jobs without worrying about the "boring parts" of building the infrastructure.

Kubeflow seeks to tackle the problem of simplifying machine learning through three features: composability, portability, and scalability.

* Composability - the core components come from data science tools that are already familiar to machine learning practictioners. They can be used independently to facilitate specific stages of machine learning, or composed together to form end-to-end pipelines.

* Portability - taking advantage of Kubernetes and cloud-native microservices, Kubeflow does not require you to anchor to any particular platform. You can develop and prototype on your laptop, and deploy to production effortlessly.

* Scalability - the container-based design allows your clusters to dynamically scale up or down according to demand.


Let's take a closer look at some of these components.

==== Training

Kubeflow supports a variety of distributed training frameworks. As of the time of this writing, Kubeflow has support for:

* TensorFlow
* PyTorch
* MXNet
* Chainer
* Caffe2
* MPI


In Kubeflow, distributed training jobs are managed by application-specific controllers, known as "operators". These operators extend the Kubernetes APIs to create, manage, and manipulate the state of resources. For example, to run a distributed TensorFlow training job, the user just needs to provide a specification that describes the desired state (number of workers and parameter servers, etc), and the TensorFlow operator component will take care of the rest and manage the lifecycle of the training job.

In a later chapter we will examine how Kubeflow trains a TensorFlow model in greater detail.

==== Serving

After training your model, the next step is to serve the model in your cluster so it can handle prediction requests. Kubeflow makes it easy for data scientists to deploy machine learning models in production environments at scale. Currently Kubeflow supports TensorFlow Serving and SeldonCore.

Serving many types of models on Kubeflow is fairly straightforward. There is no need to build or customize a container yourself - simply point Kubeflow to where your model is stored, and a server will be ready within seconds to service requests.

==== Hyperparameter Tuning

In machine learning, hyperparameters are variables that govern the training process. For example, what should the model's learning rate be? How many hidden layers and neurons should be in the neural network? These parameters are not part of the training data, but they can have a significant effect on the performance of the training models.

Finding the right set of hyperparameters for your training model can be a challenging task. Traditional methodologies such as grid search can be time-consuming and quite tedious.

Kubeflow provides a component (called "Katib") that allows users to perform hyperparameter optimizations easily on Kubernetes clusters. Katib is inspired by Vizier, a black-box optimization framework created at Google. It leverages advanced searching algorithms such as Bayesian optimization to find optimal hyperparameter configurations.

With Kubeflow, users can begin with a training model that they are unsure about, define the hyperparameter search space, and Kubeflow will take care of the rest - spin up training jobs using different hyperparameters, collect the metrics, and save the results to a model database so their performance can be compared.


==== Jupyter Notebooks

Jupyter notebook is an open-source web application that allows users to create and share data, code snippets, and experiments at scale. They are popular among machine learning practitioners due to their simplicity and portability.

.A Jupyter notebook running in Kubeflow
[#1_jupyter_notebook]
image::images/1_jupyter_notebook.png[A Jupyter notebook running in Kubeflow]

In Kubeflow, you can spin up instances of Jupyter notebooks that directly interact with your cluster and its other components. For example, you can write snippets of TensorFlow distributed training code in your laptop, and bring up a training cluster with just a few clicks.

==== Pipelines

A machine learning pipeline can be seen as a graph, where each node is a stage in a workflow. Kubeflow Pipelines is a component that allows users to compose reusable workflows at ease. Its features include:

* An orchestratration engine for multi-step workflows

* An SDK to interact with pipeline components

* A user interface that allows users to visualize and track experiments, and to share results with collaborators


.A Kubeflow pipeline
[#1_kubeflow_pipeline]
image::images/1_kubeflow_pipeline.png[A Kubeflow pipeline]

Each building block in a pipeline is a self-contained step in a machine learning workflow, such as preprocessing, transformation, training, or cross-validation. In Kubeflow, each of these blocks is built as a Docker image. This allows users to share and reuse individual components with ease.

==== Dataprep

Machine learning requires good data to be effective, and often special tools are used to get the data.
Just getting the data can be part of the challenge, often you also need to do extra work to filter, normalize and prepare your data for machine learning.
Kubeflow supports a few different tools for this:

* Apache Spark (one of the most popular big data tools)

* Pachyderm

* Tensorflow Transform (easy integration into model serving)

=== Getting Set Up with Kubeflow

One of the great things about Kubeflow is the ability to do our initial development and exploration locally, moving into more powerful and distributed tools later on.


[TIP]
====
While you can get started with Kubeflow locally, you don't have to - you can just as easily do your initial work with one of the cloud providers or on-prem Kubernetes clusters covered in <<appendix_installing>>.

The fastest way to get started with Kubeflow is using the simplified click-to-deploy app on Google Cloud Platform (GCP). If you're in a rush to get started go ahead and jump to <<gcp_click_to_deploy>>.
====


[[general_set_up_kubeflow]]
==== Installing Kubeflow & its dependencies

The biggest requirement for Kubeflow is access to a Kubernetes cluster, be it local (like Minikube, MicroK8s) or a remote cluster.
In addition to a Kubernetes cluster you need some extra tools, but less than you would expect since the containers contain all there own dependencies.


Kubeflow ships with scripts to automate the set up of the system dependencies as well as a local Kubernetes cluster, called Minikube.
Canonical separately provides a script which does the same, except with Micro K8s instead of Minikube.
You don't need to install Kubeflow with these scripts, but the automatic setup tools can help you get up and running faster even if you don't plan on using Minikube.

[TIP]
====
Regardless of if you use a local or remote Kubernetes cluster, having the development tools installed locally will simplify your life.
====


===== Automatically

Kubeflow's dependencies can be automatically installed along with, optional, Minikube by running the install shell script <<mini_kube_setup_sh>>.
If you prefer MicroK8s, Canonical Labs has their own auto-install script covered in <<setting_up_microk8s>>.
Even if you don't plan on using Minikube or MicroK8s, these scripts simplify the rest of the set up.

[[mini_kube_setup_sh]]
.Automatically install Kubeflow's dependencies & Minikube
====
[source, bash]
----
include::examples/dev-setup/install-kf-and-minikube.sh[tags=work]
----
====

[TIP]
====
If this fails to install minikube, don't worry we'll cover how to fix that up in <<setting_up_minikube>>.
====



[WARNING]
====
These scripts will modify your system packages installing and require root/super user permissions. If you wish you can instead run these commands on another host (e.g. dedicated KF development enviroment).
====

===== Manually

// TODO(holden): This is going to change

If you don't want to install Minikube, you can manually install Kubeflow's dependencies: ksonnet and kubectl.
// TODO(holden): Where to check the versions
Ksonnet is on github, with its release posted at link:$$https://github.com/ksonnet/ksonnet/releases$$[https://github.com/ksonnet/ksonnet/releases], and you can install it with <<install_ksonnet_sh>>.

[NOTE]
====
Even if you have these installed, you will want to check that you have the right versions installed for your version of Kubeflow.
====

[[install_ksonnet_sh]]
.Install ksonnet
====
[source, bash]
----
include::examples/dev-setup/install-ksonnet-and-kubectl.sh[tags=ksonnet]
----
====



Kubectl is more widely packaged, with the different installation options covered in the link:$$https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl$$[kubernetes documentation https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl].
If you want to use a package manager to install kubectl,
Ubuntu users can use snap <<install_kubectl_snap>> and
Homebrew users can install Kubectl <<install_kubectl_homebrew>>,
the simplest with package manager for this, with others covered in link:$$https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl$$[the guide].
Kubectl can also be installed outside of your package manager <<install_kubectl_curl>>, although this requires manual updating.

[[install_kubectl_snap]]
.Install kubectl with snap
====
[source, bash]
----
include::examples/dev-setup/install-ksonnet-and-kubectl.sh[tags=ubuntu-kubectl]
----
====

[[install_kubectl_homebrew]]
.Install kubectl with homebrew
====
[source, bash]
----
include::examples/dev-setup/install-ksonnet-and-kubectl.sh[tags=osx-kubectl]
----
====

[[install_kubectl_curl]]
.Install kubectl without a package manager and no sudo
====
[source, bash]
----
include::examples/dev-setup/install-ksonnet-and-kubectl.sh[tags=no-pkg-manager-kubectl]
----
====



Once you have the dependencies installed, you can now install Kubeflow as in <<install_kf>>.

[[install_kf]]
.Install Kubeflow
====
[source, bash]
----
include::examples/dev-setup/install-kf.sh[tags=install]
----
====


You now have Kubeflow installed on your machine. Now we will cover some additional optional tools that you can set up to make the rest of this book easier.


==== Setting up Kubeflow for local use

Being able to have the same software running locally as in production is one of the great advantages of Kubeflow.
To support this you will need a local version of Kubernetes installed.
Kubeflow has setup scripts which automatically download and attempt to set up Minikube, but this is not your only option.
Ubuntu users in particular may find MicroK8s faster to set up.


[TIP]
====
You don't need to have a local Kubernetes cluster, but many data scientists and developers find having a local cluster to test with (be it Minikube or MicroK8s or other).
====

[[set_up_multipass]]
===== Setting up Multipass

This part is optional, however you may find it easier to have a throw away system to do your learning in.
Multipass (https://github.com/CanonicalLtd/multipass) is a system for orchestrating virtual machines in Linux or OSX.
If you don't want multi-pass, feel free to skip ahead.

[TIP]
====
Multipass is also very useful for OSX because the system set up can be more challanging, and multipass gives you a Linux enviroment to work in
====


To install `multipass` on OSX, go to https://github.com/CanonicalLtd/multipass/releases and download `*-Darwin.pkg` from
the latest release.  Follow the instructions in the installer.

To install `multipass` in a Linux environment using `snap` run the following at the command line interface:

```
sudo snap install multipass --beta --classic
```

====== Create Machine and Setup `microk8s`

Now that you have multipass set up, you will want to make a machine you can expirement in.

```
multipass launch bionic -n kubeflow -m 8G -d 40G -c 4
```

This will create a VM based on `bionic-beaver` also known as Ubuntu 18.04.  We are naming the machine `kubeflow` and
alloting it 8 GB of memory, 40 GB of disk space and 4 cpu cores. You may need to reduce those amount on smaller systems.

Now that you have this VM created you can get a shell to it and start following the normal set up directions.


```
multipass shell kubeflow
```

To allow your machine to be able to connect to the web ui and other parts of the console you will need to change the iptables rules with:

```
sudo iptables -P FORWARD ACCEPT
```


[[setting_up_minikube]]
===== Minikube

Minikube is a local version of Kubernetes which can be used in conjunction with Kubeflow.

The set up script for Kubeflow we showed earlier will attempt to install Minikube so you can work locally, although it may sometimes not succeed.
// TODO: other local k8s options



The most common reason for the automatic set up of Minikube as part of the set-up scripts failing is missing a hypervisor or Docker install.
The regardless of your OS you should be able to use link:$$https://www.virtualbox.org/wiki/Downloads$$[VirtualBox], however other options like KVM on Linux, Hyper-V on Windows, and HyperKit on MacOs all work as well.
There are installation guides for Minikube on the main Kubernetes website link:$$https://kubernetes.io/docs/tasks/tools/install-minikube/$$[] as well as the Kubeflow specific one at link:$$https://www.kubeflow.org/docs/started/getting-started-minikube/$$[].



[TIP]
====
The Kubeflow project has a link:$$https://www.kubeflow.org/docs/started/getting-started-minikube/$$[frequently updated guide on how to install Kubeflow with Minikube at https://www.kubeflow.org/docs/started/getting-started-minikube/].
====

[[setting_up_microk8s]]
===== MicroK8s

link:$$https://microk8s.io/$$[MicroK8s] is another single node Kubernetes option.
Ubuntu users can directly install MicroK8s from snap with <<install_microk8s>>.

[[install_microk8s]]
.Install MicroK8s
====
[source, bash]
----
include::examples/dev-setup/install-microk8s.sh[tags=installmicrok8s]
----
====

MicroK8s requires some post installation set-up work.
First off, if you want to use it's built in docker registry your going to want to turn it on and configure the host.
Also, it ships with special versions of commands, including the `kubectl` and `docker` commands, which are accessed with as `microk8s.[command]`.
Since Kubeflow uses the standard names, you will want to add an alias as shown in <<setup_microk8s>>

[[setup_microk8s]]
.Set up MicroK8s components and aliases
====
[source, bash]
----
include::examples/dev-setup/install-microk8s.sh[tags=setupmicrok8s]
----
====

Once you're done using MicroK8s, or you otherwise want to access the regular versions of the commands you remove the alias as in <<unalias_microk8s>>.

[[unalias_microk8s]]
.Set up MicroK8s components and aliases
====
[source, bash]
----
include::examples/dev-setup/install-microk8s.sh[tags=unaliasmicrok8s]
----
====



Once you have MicroK8s installed you can use the Canonical provided bootstrap scripts to set up Kubeflow on top of it <<bootstrap_microk8s_kf>>,
or use it like a regular K8s cluster.



.Bootstrap Kubeflow on MicroK8s
[[bootstrap_microk8s_kf]]
====
[source, bash]
----
include::examples/dev-setup/install-microk8s.sh[tags=bootstrapwithcanonicallabs]
----
====




==== Setting up your Kubeflow development enviroment

In addition to the core dependencies you may find some additional tools useful.
Much of Kubernets and Kubeflow is configured with
either link:$$https://yaml.org/$$[YAML Ain't Markup Language (YAML)]
or link:$$https://ksonnet.io/$$[ksonnet] (which is an extension of link:$$https://jsonnet.org/$$[Jsonnet]).

[TIP]
====
This part is optional, if it ends up being frustrating or slowing you down feel free to skip ahead.
====


Inside the Kubeflow project, the Jsonnet library is used to automatically format Ksonnet into a standard layout. This autoformatter can also catch syntax errors or other simple errors earlier on in the deploy.
You can install from jsonnet package link:$$https://github.com/google/jsonnet/releases$$[releases are available on GitHub] <<jsonnet_manual>>, link:$$https://snapcraft.io/$$[Snap] <<jsonnet_snap>> (from Canonical).

[WARNING]
====
While jsonnet is listed on PyPi, it doesn't install the command line tools which are needed by some IDEs.
====


[[jsonnet_snap]]
.Install jsonnet with snap (Ubuntu)
====
[source, bash]
----
include::examples/dev-setup/jsonnet.sh[tags=snap]
----
====

[[jsonnet_manual]]
.Install jsonnet manually (everyone else)
====
[source, bash]
----
include::examples/dev-setup/jsonnet.sh[tags=manual]
----
====


Most Integrated Development Enviroments (IDEs) offer some sort of tooling for editing YAML and Jsonnet, but you may have to install these seperately.
For InteliJ //TODO(trevor)
For emacs there are many modes available for YAML editing, as well as the link:$$https://github.com/mgyucht/jsonnet-mode$$[jsonnet-mode] (which is installable from link:$$https://melpa.org/$$[Milkypostman’s Emacs Lisp Package Archive (MELPA)]).
Atom has syntax highlighting available as packages for both link:$$https://atom.io/packages/language-yaml$$[YAML] and link:$$https://atom.io/packages/language-jsonnet$$[Jsonnet].
If you use a different IDE, don't throw it away just for better ksonnet editing before you explore the plugin available.

[TIP]
====
If you get stuck here, or if your permissions/policies don't allow you to use minikube, just jump on over to <<appendix_installing>> and you can deploy your Kubeflow apps against your remote K8s cluster.
====


[[brief_alternatives_to_kubeflow]]
=== Brief Alternatives to Kubeflow


Within the research community, various alternatives exist that provide uniquely different functionality to that of Kubeflow.
In recent research cadences, model development or training has received much attention and support with regards to advancements in infrastructure, theory, and systems.
Prediction serving, on the other hand, has received relatively less attention.
As such, it is commonly seen that data science practitioners end up hacking together an amalgam of critical systems components that are integrated to support serving and inference across various workloads and continuously evolving frameworks.
However, given the demand for constant availability and horizontal scalability, solutions like Kubeflow and various others are gaining traction through industry as powerful architectural abstraction tools and as convincing research scopes.

==== Clipper (RiseLabs)

[#6_clipper]
image::images/6_clipper.png[Clipper and its modular architecture]
One interesting alternative to Kubeflow, is a general-purpose low-latency prediction serving system that came out of RiseLabs, titled Clipper. In an attempt to simplify deployment, optimization, and inference, Clipper has developed a layered architecture system like the one seen above.
Through various optimizations and its modular design, Clipper, in its seminal paper, has achieved low latency and high throughput predictions at comparable levels to TensorFlow Serving, on three TensorFlow models of varying inference costs.

Clipper is divided across two abstractions, aptly named: model selection and model abstraction layers.

The model selection layer is quite sophisticated in that it uses an adaptive online model selection policy and various ensemble techniques to incorporate feedback and automatically select and combine predictions
from models that can span multiple machine learning frameworks. Since you are continuously learning from feedback throughout the lifetime of your application, the model selection layer self-calibrates failed model without needing to interact directly with the policy layer.
The two different selection policies that are implemented allow for single-model selection, using a multi-armed bandit algorithm, and ensemble-model selection, using bootstrap aggregation (or bagging).

The model abstraction layer provides an implementable interface for providing pluggability in terms of custom machine learning frameworks.
The layer is comprised of a prediction cache, an adaptive query-batching component, and model containers that are communicable through RPC.
These model containers are built as Docker containers, so it was intended that Clipper may run on Kubernetes seamlessly.

Clipper's modular architecture and focus on containerization, similar to Kubeflow, enables caching and batching mechanisms to be shared across frameworks while also reaping the gains of scalability, concurrency, and flexibility in adding new model frameworks.

Graduating from just scientific theory into a functional end-to-end system, Clipper has gained traction within the scientific community and has had various parts of its architectural designs incorporated into recently introduced machine learning systems; nonetheless, we have yet to see if it will be adopted in industry at scale.

==== MLflow (Databricks)

MLflow is Databrick's own flavor of open-source machine learning development.
The architecture of MLflow leverages a lot of the same architectural paradigms of Clipper, including its framework agnostic nature,  while focusing on three major components that it calls: Tracking, Projects, and Models.

MLflow Tracking functions as an API with a complementing UI for logging parameters, code versions, metrics and output files. This is quite powerful in machine learning as tracking parameters, metrics, and artifacts is of paramount importance.
The power of this API is that it can be leveraged in a standalone or notebook setting.

MLflow Projects provides a standard format for packaging reusable data science code, defined by a yaml file that can leverage source-controlled code and dependency management via Anaconda.
The project format makes it easy to share reproducible data science code, as reproduciblity is critical for machine learning practitioners.

MLflow Models is a convention for packaging machine learning models in multiple formats.
Each MLflow Model is saved as a directory containing arbitrary files and an MLmodel descriptor file.

MLflow is still in active development, but with the entire project being open-sourced, the visibility and community excitement around its development is enticing as well.

==== Others

Because of the challenges presented in machine learning development, many organizations have started to build internal machine learning platforms to manage their machine learning lifecycle.
For example: Bloomberg, Facebook, Google, Uber have built the Data Science Platform, FBLearner Flow, TFX, and Michelangelo to manage data preparation, model training and deployment.
With the machine learning infrastructure landscape always evolving and maturing, we are excited to see how open-source projects, like Kubeflow, will bring much-needed simplicity and abstraction to machine learning development.

=== Getting Help / Community Resources

We are so glad you've decided to use this book to start your adventures into Kubeflow.
However, like all adventures, there may come a point when your guide book isn't enough to carry you through.
Thankfully there are a collection of community resources where you can interact with others on similar adventures to your own.
We encourage you to sign up for the link:$$http://kubeflow.slack.com$$[Kubeflow slack at http://kubeflow.slack.com], one of the more active areas of discussion, now since account approval is semi-manual.
// TODO(holden): Check and see if we split the user/dev list out from discuss@
While less active than slack, there is also a mailing list at link:$$https://groups.google.com/forum/#!forum/kubeflow-discuss$$[].
The link:$$https://www.kubeflow.org/$$[kubeflow project page is at https://www.kubeflow.org/].

[TIP]
====
If you want to quickly explore Kubeflow end-to-end there are some link:$$https://codelabs.developers.google.com/$$[Google codelabs] with Kubeflow as well.
====

=== Conclusion

At this point you have everything you need to get started on your Kubeflow adventure: a Kubernetes cluster, Kubeflow and its core dependencies, and a desire to have fun.
In the next chapter <<simple_training_ch>>, we'll train and serve a relatively simple machine learning model together to give you an idea of how the basics of how to use Kubeflow.
